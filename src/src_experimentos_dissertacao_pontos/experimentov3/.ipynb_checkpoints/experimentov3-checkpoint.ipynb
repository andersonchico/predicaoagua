{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b102e69578a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "import os \n",
    "#sys.path.append('../../')\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "from numpy import concatenate\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM,Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from utils import utilidades as myutils\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_group_data(dataset, parameter, range_upper = 1,range_lower = 1):\n",
    "    \n",
    "    q1, q3 = np.percentile(dataset[parameter],[25,75])\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q3 - range_upper*(1.5 * iqr)  \n",
    "    upper_bound = q3 + range_upper*(1.5 * iqr)  \n",
    "\n",
    "    # Deleting lower bound and upper bound from the dataset LinkTT2\n",
    "    dataset = dataset.loc[(dataset[parameter] >= lower_bound) & \n",
    "                                              (dataset[parameter] <= upper_bound)]\n",
    "    #dataset.interpolate()\n",
    "        \n",
    "    return dataset\n",
    "\n",
    "def resultado(df,tecnica,parameter,amostras,metrica,lag):\n",
    "    \n",
    "    df.loc[len(df)]= [tecnica,parameter,amostras,round(metrica,2),lag]\n",
    "    \n",
    "    \n",
    "    return df\n",
    "\n",
    "#Prepara o conjunto de dados em X e y, considerando a janela de visualização (lags).\n",
    "#cy = coluna que será predita\n",
    "def prepara_dados(dados,lags,cy):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(lags, len(dados)):\n",
    "        X.append(dados[i-lags:i,:])\n",
    "        y.append(dados[i, cy])\n",
    "       \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def pegar_dados_coluna_predita_train_test(trainingd,percent,index_coluna):\n",
    "    data = trainingd.iloc[:,:].values\n",
    "    train = trainingd.iloc[0:int(len(data)*percent),index_coluna:index_coluna+1].values\n",
    "    train_previsao = trainingd.iloc[0:int(len(data)*percent),index_coluna:index_coluna+1].values\n",
    "    test = trainingd.iloc[len(train):,index_coluna:index_coluna+1].values \n",
    "    '''\n",
    "    print('Nº observações:', len(data))\n",
    "    print('treino:',len(train))\n",
    "    print('teste:',len(test))\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    return train,train_previsao, test\n",
    "\n",
    "#Normalização dos dados: Normaliza os dados dentro um intervalo (0 a 1).\n",
    "def normalizacao(train,test):\n",
    "    sc = MinMaxScaler()\n",
    "    testd = test\n",
    "    train = sc.fit_transform(train)\n",
    "    test = sc.fit_transform(test)    \n",
    "    return train,test,testd\n",
    "\n",
    "def tranformacao_log(valores_parametro):\n",
    "   \n",
    "    valores_log = []\n",
    "   \n",
    "    for i in range(len(valores_parametro)):\n",
    "        #print(np.log(np.array(valores_parametro[i]) + 0.000000001))\n",
    "        valores_log.append(np.log(np.array(valores_parametro[i]) + 0.000000001))\n",
    "       \n",
    "    return valores_log\n",
    "\n",
    "def cria_dataframe_obsprevisoes(data_coleta,observado,previsao):\n",
    "    \n",
    "    dict = {'data_coleta':data_coleta,'observado': observado, 'previsao': previsao}\n",
    "    \n",
    "    dfobsprev = pd.DataFrame(dict)\n",
    "    \n",
    "    return dfobsprev\n",
    "\n",
    "def ajusta_lista(array):\n",
    "    lista = []\n",
    "    \n",
    "    for i in range(len(array)):\n",
    "        lista.append(array[i][0])\n",
    "        \n",
    "    #print('ajusta array:',lista)\n",
    "        \n",
    "    return lista\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tecnica</th>\n",
       "      <th>parametro</th>\n",
       "      <th>amostras</th>\n",
       "      <th>mape</th>\n",
       "      <th>lag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Regressão Linear</td>\n",
       "      <td>Coliformes Termotolerantes</td>\n",
       "      <td>151</td>\n",
       "      <td>55.41</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Coliformes Termotolerantes</td>\n",
       "      <td>151</td>\n",
       "      <td>55.31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP</td>\n",
       "      <td>Coliformes Termotolerantes</td>\n",
       "      <td>151</td>\n",
       "      <td>38.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>Coliformes Termotolerantes</td>\n",
       "      <td>151</td>\n",
       "      <td>40.04</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Regressão Linear</td>\n",
       "      <td>Coliformes Termotolerantes</td>\n",
       "      <td>151</td>\n",
       "      <td>41.82</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>pH</td>\n",
       "      <td>179</td>\n",
       "      <td>1.43</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>Regressão Linear</td>\n",
       "      <td>pH</td>\n",
       "      <td>179</td>\n",
       "      <td>1.66</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>pH</td>\n",
       "      <td>179</td>\n",
       "      <td>1.71</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>MLP</td>\n",
       "      <td>pH</td>\n",
       "      <td>179</td>\n",
       "      <td>1.73</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>pH</td>\n",
       "      <td>179</td>\n",
       "      <td>1.45</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>320 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              tecnica                   parametro amostras   mape lag\n",
       "0    Regressão Linear  Coliformes Termotolerantes      151  55.41   1\n",
       "1       Random Forest  Coliformes Termotolerantes      151  55.31   1\n",
       "2                 MLP  Coliformes Termotolerantes      151  38.88   1\n",
       "3                LSTM  Coliformes Termotolerantes      151  40.04   1\n",
       "4    Regressão Linear  Coliformes Termotolerantes      151  41.82   2\n",
       "..                ...                         ...      ...    ...  ..\n",
       "315              LSTM                          pH      179   1.43   9\n",
       "316  Regressão Linear                          pH      179   1.66  10\n",
       "317     Random Forest                          pH      179   1.71  10\n",
       "318               MLP                          pH      179   1.73  10\n",
       "319              LSTM                          pH      179   1.45  10\n",
       "\n",
       "[320 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = ['coliformes', 'od', 'turbidez', 'fosforo', 'solido', 'dbo', 'temperatura', 'ph']\n",
    "df_resultado = pd.DataFrame(columns=['tecnica','parametro','amostras','mape','lag'])\n",
    "\n",
    "DFmerge = pd.DataFrame()\n",
    "for parameter in parameters:\n",
    "    # Concatenating all the parameter files\n",
    "    df = pd.read_csv('../data/CETESB/'+ parameter + '.csv',encoding='utf-8',sep=';')\n",
    "    DFmerge = pd.concat([DFmerge, df])\n",
    "    \n",
    "    \n",
    "for parameter in DFmerge['parametro'].unique():    \n",
    "    DFmerged = DFmerge[(DFmerge['codigo_ponto'] == 'TIET02050') &\n",
    "                      (DFmerge['parametro'] == parameter)]  \n",
    "    \n",
    "    #print(parameter)\n",
    "     \n",
    "    DFmerged = clean_group_data(DFmerged, 'valor')\n",
    "    \n",
    "    DFmerged['data_coleta'] = pd.to_datetime(DFmerged['data_coleta'])\n",
    "    DFmerged.index = DFmerged['data_coleta']    \n",
    "    DFmerged = DFmerged.groupby([pd.Grouper(freq='1M'), 'parametro'])['valor'].mean().unstack()\n",
    "    DFmerged = DFmerged.loc['1979-01-31':]\n",
    "    \n",
    "    DFmerged = DFmerged.interpolate(method = 'pad')\n",
    "    \n",
    "    dfnovo = DFmerged\n",
    "    dfnovo = dfnovo.reset_index()\n",
    "    \n",
    "    DFmerged[parameter] = tranformacao_log(DFmerged[parameter])\n",
    "        \n",
    "    # #Mostra a quantidade de NaN no dataframe\n",
    "    #for field in DFmerge.columns:\n",
    "        #print(field, 'NaN:', DFmerge[field].isnull().sum())\n",
    "    #print(parameter)    \n",
    "    #print('Valor', 'NaN:', DFmerge['valor'].isnull().sum())\n",
    "    \n",
    "    #seleciona os dados\n",
    "    \n",
    "    train,train_previsao, test = pegar_dados_coluna_predita_train_test(DFmerged,0.70,0)\n",
    "    y_test = test\n",
    "        \n",
    "    for lags in range(1,11):\n",
    "        \n",
    "        #normalização dos dados\n",
    "        scaler = MinMaxScaler()\n",
    "        train,test,testd = normalizacao(train,test)\n",
    "                \n",
    "        #Prepara os dados de treinamento\n",
    "        train_X,train_y = prepara_dados(train, lags,0)  \n",
    "        train_X_lstm = train_X\n",
    "        train_y_lstm = train_y\n",
    "                \n",
    "        #Ajusta a dimensão de train_X \n",
    "        nsamples, nx, ny = train_X.shape\n",
    "        train_X = train_X.reshape((nsamples,nx*ny))\n",
    "    \n",
    "        #Prepara os dados de teste\n",
    "        entradas = DFmerged[len(DFmerged) - len(test) - lags:].values\n",
    "        entradas = scaler.fit_transform(entradas)  \n",
    "    \n",
    "        test_X = []\n",
    "        for i in range(lags, lags+len(test)):        \n",
    "            test_X.append(entradas[i-lags:i, 0:1])\n",
    "        test_X = np.array(test_X)\n",
    "        \n",
    "        test_X_lstm = test_X\n",
    "                \n",
    "        #Ajusta a dimensão de test_X e train_X\n",
    "        nsamples, nx, ny = test_X.shape\n",
    "        test_X = test_X.reshape((nsamples,nx*ny))\n",
    "                        \n",
    "        '''\n",
    "        dataset = pd.DataFrame(np.concatenate([X,y], axis=1))\n",
    "        dataset.columns = ['X','y']\n",
    "        dataset.dropna(inplace=True)\n",
    "        '''\n",
    "                                \n",
    "        #Regressão Linear\n",
    "        model = LinearRegression(normalize=False)\n",
    "\n",
    "        model.fit(train_X,train_y)\n",
    "        \n",
    "        #Dados de teste\n",
    "        previsoes = model.predict(test_X).reshape(-1, 1) \n",
    "        previsoes = scaler.inverse_transform(previsoes)\n",
    "        '''\n",
    "        data = dfnovo.iloc[len(train):,0:1].values \n",
    "        data = ajusta_lista(data)\n",
    "        observado = ajusta_lista(y_test)\n",
    "        previsao  = ajusta_lista(previsoes)\n",
    "        '''\n",
    "        #print('y_test:',len(y_test))\n",
    "        #print('previsoes:',len(previsoes))\n",
    "               \n",
    "        df_result = resultado(df_resultado,'Regressão Linear',parameter,len(DFmerged),\n",
    "                              myutils.mean_absolute_percentage_error(y_test[:len(previsoes)], previsoes),lags)\n",
    "        '''\n",
    "        dfprevisoes = cria_dataframe_obsprevisoes(data,observado,previsao)\n",
    "        \n",
    "        dfprevisoes.to_csv (r'/home/anderson/Downloads/predicaoagua/src/previsoes/temporal_rl_lag'+str(lags)+parameter+'.csv', index = True, header=True)\n",
    "        '''\n",
    "        #Random Forest\n",
    "        model = RandomForestRegressor()\n",
    "\n",
    "        model.fit(train_X,train_y)\n",
    "        \n",
    "        #Dados de teste\n",
    "        previsoes = model.predict(test_X).reshape(-1, 1)\n",
    "        previsoes = scaler.inverse_transform(previsoes)\n",
    "        '''\n",
    "        data = dfnovo.iloc[len(train):,0:1].values \n",
    "        data = ajusta_lista(data)\n",
    "        observado = ajusta_lista(y_test)\n",
    "        previsao  = ajusta_lista(previsoes)\n",
    "        '''\n",
    "        df_result = resultado(df_resultado,'Random Forest',parameter,len(DFmerged),\n",
    "                              myutils.mean_absolute_percentage_error(y_test[:len(previsoes)], previsoes),lags)\n",
    "        '''\n",
    "        dfprevisoes = cria_dataframe_obsprevisoes(data,observado,previsao)\n",
    "        \n",
    "        dfprevisoes.to_csv (r'/home/anderson/Downloads/predicaoagua/src/previsoes/temporal_rf_lag'+str(lags)+parameter+'.csv', index = True, header=True)\n",
    "        '''\n",
    "        \n",
    "        #MLP\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units = 10, activation = 'relu', input_dim = train_X.shape[1]))\n",
    "        model.add(Dense(units = 21, activation = 'relu'))\n",
    "        model.add(Dense(units = 1, activation = 'relu'))\n",
    "        model.compile(loss = 'mean_absolute_error', optimizer = 'adam',metrics = ['mean_absolute_error'])\n",
    "\n",
    "        es = EarlyStopping(monitor='val_loss', patience = 5, verbose=0)\n",
    "\n",
    "        #Treina o modelo\n",
    "        history = model.fit(train_X,train_y, validation_data = (train_X,train_y),  batch_size = 4, epochs = 2000, callbacks=[es], verbose=0)\n",
    "\n",
    "        #Dados de teste\n",
    "        media_previsoes = []\n",
    "        for r in range(0,6):\n",
    "            previsoes = model.predict(test_X)\n",
    "            previsoes = scaler.inverse_transform(previsoes)\n",
    "\n",
    "            media_previsoes.append(myutils.mean_absolute_percentage_error(y_test[:len(previsoes)], previsoes))  \n",
    "            \n",
    "        '''\n",
    "        data = dfnovo.iloc[len(train):,0:1].values \n",
    "        data = ajusta_lista(data)\n",
    "        observado = ajusta_lista(y_test)\n",
    "        previsao  = ajusta_lista(previsoes)\n",
    "        \n",
    "        dfprevisoes = cria_dataframe_obsprevisoes(data,observado,previsao)\n",
    "        \n",
    "        dfprevisoes.to_csv (r'/home/anderson/Downloads/predicaoagua/src/previsoes/temporal_MLP_lag'+str(lags)+parameter+'.csv', index = True, header=True)\n",
    "        '''\n",
    "\n",
    "        df_result = resultado(df_resultado,'MLP',parameter,len(DFmerged),np.mean(media_previsoes),lags)\n",
    "\n",
    "        \n",
    "        #LSTM\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(units = 10, input_shape = (train_X_lstm.shape[1], 1)))\n",
    "        model.add(Dense(21, activation = 'relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(1, activation = 'relu'))\n",
    "        model.compile(loss = 'mean_absolute_error', optimizer = 'adam',metrics = ['mean_absolute_error'])\n",
    "\n",
    "        es = EarlyStopping(monitor='val_loss', patience = 5, verbose=0)\n",
    "\n",
    "        #Treina o modelo\n",
    "        history = model.fit(train_X_lstm,train_y_lstm,validation_data = (train_X_lstm,train_y_lstm), batch_size = 4, epochs = 100, callbacks=[es], verbose=0)\n",
    "\n",
    "        #Dados de teste\n",
    "        media_previsoes = []\n",
    "        for r in range(0,6):\n",
    "            previsoes = model.predict(test_X_lstm)\n",
    "            previsoes = scaler.inverse_transform(previsoes)\n",
    "            media_previsoes.append(myutils.mean_absolute_percentage_error(y_test[:len(previsoes)], previsoes))\n",
    "        '''\n",
    "        data = dfnovo.iloc[len(train):,0:1].values \n",
    "        data = ajusta_lista(data)\n",
    "        observado = ajusta_lista(y_test)\n",
    "        previsao  = ajusta_lista(previsoes)\n",
    "        \n",
    "        dfprevisoes = cria_dataframe_obsprevisoes(data,observado,previsao)\n",
    "        \n",
    "        dfprevisoes.to_csv (r'/home/anderson/Downloads/predicaoagua/src/previsoes/temporal_LSTM_lag'+str(lags)+parameter+'.csv', index = True, header=True)\n",
    "        '''\n",
    "        df_result = resultado(df_resultado,'LSTM',parameter,len(DFmerged),np.mean(media_previsoes),lags)\n",
    "\n",
    "        #77.24\n",
    "        #60.56\n",
    "        #42.73\n",
    "    \n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.to_csv (r'/home/anderson/Downloads/predicaoagua/src/temporal_pad_lag_limpo_relu.csv', index = True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predição espaço-temporal\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tecnica</th>\n",
       "      <th>parametro</th>\n",
       "      <th>amostras</th>\n",
       "      <th>mape</th>\n",
       "      <th>lag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Regressão Linear</td>\n",
       "      <td>Coliformes Termotolerantes</td>\n",
       "      <td>137</td>\n",
       "      <td>32.85</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Coliformes Termotolerantes</td>\n",
       "      <td>137</td>\n",
       "      <td>32.98</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP</td>\n",
       "      <td>Coliformes Termotolerantes</td>\n",
       "      <td>137</td>\n",
       "      <td>29.82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>Coliformes Termotolerantes</td>\n",
       "      <td>137</td>\n",
       "      <td>21.02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Regressão Linear</td>\n",
       "      <td>Coliformes Termotolerantes</td>\n",
       "      <td>137</td>\n",
       "      <td>40.84</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>pH</td>\n",
       "      <td>161</td>\n",
       "      <td>0.87</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>Regressão Linear</td>\n",
       "      <td>pH</td>\n",
       "      <td>161</td>\n",
       "      <td>0.92</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>pH</td>\n",
       "      <td>161</td>\n",
       "      <td>0.99</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>MLP</td>\n",
       "      <td>pH</td>\n",
       "      <td>161</td>\n",
       "      <td>2.44</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>pH</td>\n",
       "      <td>161</td>\n",
       "      <td>0.88</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>320 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              tecnica                   parametro amostras   mape lag\n",
       "0    Regressão Linear  Coliformes Termotolerantes      137  32.85   1\n",
       "1       Random Forest  Coliformes Termotolerantes      137  32.98   1\n",
       "2                 MLP  Coliformes Termotolerantes      137  29.82   1\n",
       "3                LSTM  Coliformes Termotolerantes      137  21.02   1\n",
       "4    Regressão Linear  Coliformes Termotolerantes      137  40.84   2\n",
       "..                ...                         ...      ...    ...  ..\n",
       "315              LSTM                          pH      161   0.87   9\n",
       "316  Regressão Linear                          pH      161   0.92  10\n",
       "317     Random Forest                          pH      161   0.99  10\n",
       "318               MLP                          pH      161   2.44  10\n",
       "319              LSTM                          pH      161   0.88  10\n",
       "\n",
       "[320 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predição espaço-temporal\n",
    "print('predição espaço-temporal')\n",
    "\n",
    "parameters = ['coliformes', 'od', 'turbidez', 'fosforo', 'solido', 'dbo', 'temperatura', 'ph']\n",
    "df_resultado = pd.DataFrame(columns=['tecnica','parametro','amostras','mape','lag'])\n",
    "\n",
    "DFmerge = pd.DataFrame()\n",
    "for parameter in parameters:\n",
    "    # Concatenating all the parameter files\n",
    "    df = pd.read_csv('../data/CETESB/'+ parameter + '.csv',encoding='utf-8',sep=';')\n",
    "    DFmerge = pd.concat([DFmerge, df])\n",
    "    \n",
    "    \n",
    "for parameter in DFmerge['parametro'].unique():    \n",
    "    \n",
    "    DFmerged1 = DFmerge[(DFmerge['codigo_ponto'] == 'TIET02050') &\n",
    "                      (DFmerge['parametro'] == parameter)]  \n",
    "    \n",
    "    DFmerged2 = DFmerge[(DFmerge['codigo_ponto'] == 'TIET02090') &\n",
    "                      (DFmerge['parametro'] == parameter)]\n",
    "\n",
    "    \n",
    "    \n",
    "    # DFmerge.groupby([pd.Grouper(freq='1M'), 'codigo_ponto']).mean().unstack()\n",
    "    # DFmerge.groupby(['codigo_ponto', 'UGRHI'])['valor'].count().unstack()\n",
    "\n",
    "    #DFmerged1 = DFmerged1[DFmerged1['codigo_ponto'] == 'TIET02050']    \n",
    "    DFmerged1 = clean_group_data(DFmerged1, 'valor')\n",
    "    \n",
    "    #DFmerged2 = DFmerged2[DFmerged2['codigo_ponto'] == 'TIET02090']    \n",
    "    DFmerged2 = clean_group_data(DFmerged2, 'valor')\n",
    "    \n",
    "     \n",
    "    \n",
    "    DFmerged1['data_coleta'] = pd.to_datetime(DFmerged1['data_coleta'])\n",
    "    DFmerged1.index = DFmerged1['data_coleta']    \n",
    "    DFmerged1 = DFmerged1.groupby([pd.Grouper(freq='1M'), 'parametro'])['valor'].mean().unstack()\n",
    "    DFmerged1 = DFmerged1.loc['1979-01-31':]\n",
    "       \n",
    "    DFmerged2['data_coleta'] = pd.to_datetime(DFmerged2['data_coleta'])\n",
    "    DFmerged2.index = DFmerged2['data_coleta']    \n",
    "    DFmerged2 = DFmerged2.groupby([pd.Grouper(freq='1M'), 'parametro'])['valor'].mean().unstack()\n",
    "    DFmerged2 = DFmerged2.loc['1979-01-31':]\n",
    "    \n",
    "    #coloca o dataframes com o mesmo tamanho\n",
    "    #DFmerged1 = DFmerged1.iloc[0:len(DFmerged2)]\n",
    "    #if (len(DFmerged1) > len(DFmerged2)):\n",
    "        #DFmerged1 = DFmerged1.iloc[0:len(DFmerged2)]\n",
    "    #else:\n",
    "        #DFmerged2 = DFmerged2.iloc[0:len(DFmerged1)]   \n",
    "    \n",
    "    \n",
    "    DFmerged1 = DFmerged1.interpolate(method = 'pad')\n",
    "    DFmerged2 = DFmerged2.interpolate(method = 'pad')\n",
    "        \n",
    "    DFmerged1[parameter] = tranformacao_log(DFmerged1[parameter])\n",
    "    DFmerged2[parameter] = tranformacao_log(DFmerged2[parameter])\n",
    "    \n",
    "    DFmerged3 = DFmerged1.merge(DFmerged2, how='inner', on = 'data_coleta')\n",
    "    \n",
    "    dfnovo = DFmerged3\n",
    "    dfnovo = dfnovo.reset_index()\n",
    "        \n",
    "    # #Mostra a quantidade de NaN no dataframe\n",
    "    #for field in DFmerge.columns:\n",
    "        #print(field, 'NaN:', DFmerge[field].isnull().sum())\n",
    "    #print(parameter)    \n",
    "    #print('Valor', 'NaN:', DFmerge['valor'].isnull().sum())\n",
    "    \n",
    "    #Dividindo dataframe\n",
    "    df2050 = DFmerged3.iloc[:,0:1]\n",
    "    df2090 = DFmerged3.iloc[:,1:2]\n",
    "    \n",
    "    \n",
    "    #seleciona os dados\n",
    "    \n",
    "    train1,train_previsao1, test1 = pegar_dados_coluna_predita_train_test(df2050,0.70,0)\n",
    "    y_test1 = test1\n",
    "    \n",
    "    train2,train_previsao2, test2 = pegar_dados_coluna_predita_train_test(df2090,0.70,0)\n",
    "    y_test2 = test2\n",
    "        \n",
    "    for lags in range(1,11):\n",
    "        \n",
    "        #normalização dos dados\n",
    "        scaler = MinMaxScaler()\n",
    "        train1,test1,testd1 = normalizacao(train1,test1)\n",
    "        train2,test2,testd2 = normalizacao(train2,test2)\n",
    "                \n",
    "        #Prepara os dados de treinamento\n",
    "        train_X1,train_y1 = prepara_dados(train1, lags,0)  \n",
    "        train_X_lstm1 = train_X1\n",
    "        train_y_lstm1 = train_y1\n",
    "        \n",
    "        train_X2,train_y2 = prepara_dados(train2, lags,0)  \n",
    "        train_X_lstm2 = train_X2\n",
    "        train_y_lstm2 = train_y2\n",
    "                \n",
    "        #Ajusta a dimensão de train_X \n",
    "        nsamples, nx, ny = train_X1.shape\n",
    "        train_X1 = train_X1.reshape((nsamples,nx*ny))\n",
    "        \n",
    "        nsamples, nx, ny = train_X2.shape\n",
    "        train_X2 = train_X2.reshape((nsamples,nx*ny))\n",
    "    \n",
    "        #Prepara os dados de teste\n",
    "        entradas1 = df2050[len(df2050) - len(test1) - lags:].values\n",
    "        entradas1 = scaler.fit_transform(entradas1)  \n",
    "    \n",
    "        test_X1 = []\n",
    "        for i in range(lags, lags+len(test1)):        \n",
    "            test_X1.append(entradas1[i-lags:i, 0:1])\n",
    "        test_X1 = np.array(test_X1)\n",
    "        \n",
    "        test_X_lstm1 = test_X1\n",
    "        \n",
    "        entradas2 = df2090[len(df2090) - len(test2) - lags:].values\n",
    "        entradas2 = scaler.fit_transform(entradas2)  \n",
    "    \n",
    "        test_X2 = []\n",
    "        for i in range(lags, lags+len(test2)):        \n",
    "            test_X2.append(entradas2[i-lags:i, 0:1])\n",
    "        test_X2 = np.array(test_X2)\n",
    "        \n",
    "        test_X_lstm2 = test_X2\n",
    "                \n",
    "        #Ajusta a dimensão de test_X e train_X\n",
    "        nsamples, nx, ny = test_X1.shape\n",
    "        test_X1 = test_X1.reshape((nsamples,nx*ny))\n",
    "        \n",
    "        nsamples, nx, ny = test_X2.shape\n",
    "        test_X2 = test_X2.reshape((nsamples,nx*ny))\n",
    "                        \n",
    "        '''\n",
    "        dataset = pd.DataFrame(np.concatenate([X,y], axis=1))\n",
    "        dataset.columns = ['X','y']\n",
    "        dataset.dropna(inplace=True)\n",
    "        '''\n",
    "                        \n",
    "        #Regressão Linear\n",
    "        model = LinearRegression(normalize=False)\n",
    "\n",
    "        model.fit(train_X1,train_y1)\n",
    "        \n",
    "        #Dados de teste\n",
    "        previsoes = model.predict(test_X2).reshape(-1, 1) \n",
    "        previsoes = scaler.inverse_transform(previsoes)\n",
    "        '''\n",
    "        data = dfnovo.iloc[len(train2):,0:1].values \n",
    "        data = ajusta_lista(data)\n",
    "        observado = ajusta_lista(y_test2)\n",
    "        previsao  = ajusta_lista(previsoes)\n",
    "        \n",
    "        dfprevisoes = cria_dataframe_obsprevisoes(data,observado,previsao)\n",
    "        \n",
    "        dfprevisoes.to_csv (r'/home/anderson/Downloads/predicaoagua/src/previsoes/espaco_temporal_rl_lag'+str(lags)+parameter+'.csv', index = True, header=True)\n",
    "        '''\n",
    "        \n",
    "        #print('y_test:',len(y_test))\n",
    "        #print('previsoes:',len(previsoes))\n",
    "               \n",
    "        df_result = resultado(df_resultado,'Regressão Linear',parameter,len(DFmerged3),\n",
    "                              myutils.mean_absolute_percentage_error(y_test2[:len(previsoes)], previsoes),lags)\n",
    "        \n",
    "        \n",
    "        #Random Forest\n",
    "        model = RandomForestRegressor()\n",
    "\n",
    "        model.fit(train_X1,train_y1)\n",
    "        \n",
    "        #Dados de teste\n",
    "        previsoes = model.predict(test_X2).reshape(-1, 1)\n",
    "        previsoes = scaler.inverse_transform(previsoes)\n",
    "        '''\n",
    "        data = dfnovo.iloc[len(train2):,0:1].values \n",
    "        data = ajusta_lista(data)\n",
    "        observado = ajusta_lista(y_test2)\n",
    "        previsao  = ajusta_lista(previsoes)\n",
    "        \n",
    "        dfprevisoes = cria_dataframe_obsprevisoes(data,observado,previsao)\n",
    "        \n",
    "        dfprevisoes.to_csv (r'/home/anderson/Downloads/predicaoagua/src/previsoes/espaco_temporal_rf_lag'+str(lags)+parameter+'.csv', index = True, header=True)\n",
    "        '''\n",
    "\n",
    "        df_result = resultado(df_resultado,'Random Forest',parameter,len(DFmerged3),\n",
    "                              myutils.mean_absolute_percentage_error(y_test2[:len(previsoes)], previsoes),lags)\n",
    "        \n",
    "        #MLP\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units = 10, activation = 'relu', input_dim = train_X1.shape[1]))\n",
    "        model.add(Dense(units = 21, activation = 'relu'))\n",
    "        model.add(Dense(units = 1, activation = 'relu'))\n",
    "        model.compile(loss = 'mean_absolute_error', optimizer = 'adam',metrics = ['mean_absolute_error'])\n",
    "\n",
    "        es = EarlyStopping(monitor='val_loss', patience = 5, verbose=0)\n",
    "\n",
    "        #Treina o modelo\n",
    "        history = model.fit(train_X1,train_y1, validation_data = (train_X2,train_y2),  batch_size = 4, epochs = 2000, callbacks=[es], verbose=0)\n",
    "\n",
    "        #Dados de teste\n",
    "        media_previsoes = []\n",
    "        for r in range(0,6):\n",
    "            previsoes = model.predict(test_X2)\n",
    "            previsoes = scaler.inverse_transform(previsoes)\n",
    "\n",
    "            media_previsoes.append(myutils.mean_absolute_percentage_error(y_test2[:len(previsoes)], previsoes))  \n",
    "\n",
    "        '''\n",
    "        data = dfnovo.iloc[len(train2):,0:1].values \n",
    "        data = ajusta_lista(data)\n",
    "        observado = ajusta_lista(y_test2)\n",
    "        previsao  = ajusta_lista(previsoes)\n",
    "        \n",
    "        dfprevisoes = cria_dataframe_obsprevisoes(data,observado,previsao)\n",
    "        \n",
    "        dfprevisoes.to_csv (r'/home/anderson/Downloads/predicaoagua/src/previsoes/espaco_temporal_mlp_lag'+str(lags)+parameter+'.csv', index = True, header=True)\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        df_result = resultado(df_resultado,'MLP',parameter,len(DFmerged3),np.mean(media_previsoes),lags)\n",
    "\n",
    "        \n",
    "        #LSTM\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(units = 10, input_shape = (train_X_lstm1.shape[1], 1)))\n",
    "        model.add(Dense(21, activation = 'relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(1, activation = 'relu'))\n",
    "        model.compile(loss = 'mean_absolute_error', optimizer = 'adam',metrics = ['mean_absolute_error'])\n",
    "\n",
    "        es = EarlyStopping(monitor='val_loss', patience = 5, verbose=0)\n",
    "\n",
    "        #Treina o modelo\n",
    "        history = model.fit(train_X_lstm1,train_y_lstm1,validation_data = (train_X_lstm2,train_y_lstm2), batch_size = 4, epochs = 100, callbacks=[es], verbose=0)\n",
    "\n",
    "        #Dados de teste\n",
    "        media_previsoes = []\n",
    "        for r in range(0,6):\n",
    "            previsoes = model.predict(test_X_lstm2)\n",
    "            previsoes = scaler.inverse_transform(previsoes)\n",
    "            media_previsoes.append(myutils.mean_absolute_percentage_error(y_test2[:len(previsoes)], previsoes))\n",
    "\n",
    "        '''\n",
    "        data = dfnovo.iloc[len(train2):,0:1].values \n",
    "        data = ajusta_lista(data)\n",
    "        observado = ajusta_lista(y_test2)\n",
    "        previsao  = ajusta_lista(previsoes)\n",
    "        \n",
    "        dfprevisoes = cria_dataframe_obsprevisoes(data,observado,previsao)\n",
    "        \n",
    "        dfprevisoes.to_csv (r'/home/anderson/Downloads/predicaoagua/src/previsoes/espaco_temporal_lstm_lag'+str(lags)+parameter+'.csv', index = True, header=True)\n",
    "        '''\n",
    "        \n",
    "        df_result = resultado(df_resultado,'LSTM',parameter,len(DFmerged3),np.mean(media_previsoes),lags)\n",
    "\n",
    "        \n",
    "    \n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.to_csv (r'/home/anderson/Downloads/predicaoagua/src/espaco_temporal_pad_lag_limpo_relu.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predição espacial\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tecnica</th>\n",
       "      <th>parametro</th>\n",
       "      <th>amostras</th>\n",
       "      <th>mape</th>\n",
       "      <th>lag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Regressão Linear</td>\n",
       "      <td>Coliformes Termotolerantes</td>\n",
       "      <td>151</td>\n",
       "      <td>28.03</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Coliformes Termotolerantes</td>\n",
       "      <td>151</td>\n",
       "      <td>26.17</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP</td>\n",
       "      <td>Coliformes Termotolerantes</td>\n",
       "      <td>151</td>\n",
       "      <td>58.01</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>Coliformes Termotolerantes</td>\n",
       "      <td>151</td>\n",
       "      <td>28.00</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Regressão Linear</td>\n",
       "      <td>Oxigênio Dissolvido</td>\n",
       "      <td>204</td>\n",
       "      <td>17.57</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Oxigênio Dissolvido</td>\n",
       "      <td>204</td>\n",
       "      <td>19.25</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MLP</td>\n",
       "      <td>Oxigênio Dissolvido</td>\n",
       "      <td>204</td>\n",
       "      <td>30.88</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>Oxigênio Dissolvido</td>\n",
       "      <td>204</td>\n",
       "      <td>17.47</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Regressão Linear</td>\n",
       "      <td>Turbidez</td>\n",
       "      <td>189</td>\n",
       "      <td>34.37</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Turbidez</td>\n",
       "      <td>189</td>\n",
       "      <td>33.86</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>MLP</td>\n",
       "      <td>Turbidez</td>\n",
       "      <td>189</td>\n",
       "      <td>58.59</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>Turbidez</td>\n",
       "      <td>189</td>\n",
       "      <td>36.43</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Regressão Linear</td>\n",
       "      <td>Fósforo Total</td>\n",
       "      <td>192</td>\n",
       "      <td>14.88</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Fósforo Total</td>\n",
       "      <td>192</td>\n",
       "      <td>15.65</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>MLP</td>\n",
       "      <td>Fósforo Total</td>\n",
       "      <td>192</td>\n",
       "      <td>63.24</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>Fósforo Total</td>\n",
       "      <td>192</td>\n",
       "      <td>15.54</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Regressão Linear</td>\n",
       "      <td>Sólido Total</td>\n",
       "      <td>205</td>\n",
       "      <td>1.53</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Sólido Total</td>\n",
       "      <td>205</td>\n",
       "      <td>1.25</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>MLP</td>\n",
       "      <td>Sólido Total</td>\n",
       "      <td>205</td>\n",
       "      <td>28.27</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>Sólido Total</td>\n",
       "      <td>205</td>\n",
       "      <td>2.52</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Regressão Linear</td>\n",
       "      <td>DBO (5, 20)</td>\n",
       "      <td>161</td>\n",
       "      <td>5.42</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>DBO (5, 20)</td>\n",
       "      <td>161</td>\n",
       "      <td>3.13</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>MLP</td>\n",
       "      <td>DBO (5, 20)</td>\n",
       "      <td>161</td>\n",
       "      <td>36.91</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>DBO (5, 20)</td>\n",
       "      <td>161</td>\n",
       "      <td>36.91</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Regressão Linear</td>\n",
       "      <td>Temperatura da Água</td>\n",
       "      <td>195</td>\n",
       "      <td>1.74</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Temperatura da Água</td>\n",
       "      <td>195</td>\n",
       "      <td>1.83</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>MLP</td>\n",
       "      <td>Temperatura da Água</td>\n",
       "      <td>195</td>\n",
       "      <td>8.45</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>Temperatura da Água</td>\n",
       "      <td>195</td>\n",
       "      <td>1.71</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Regressão Linear</td>\n",
       "      <td>pH</td>\n",
       "      <td>179</td>\n",
       "      <td>1.11</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>pH</td>\n",
       "      <td>179</td>\n",
       "      <td>1.27</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>MLP</td>\n",
       "      <td>pH</td>\n",
       "      <td>179</td>\n",
       "      <td>1.09</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>pH</td>\n",
       "      <td>179</td>\n",
       "      <td>1.08</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tecnica                   parametro amostras   mape lag\n",
       "0   Regressão Linear  Coliformes Termotolerantes      151  28.03    \n",
       "1      Random Forest  Coliformes Termotolerantes      151  26.17    \n",
       "2                MLP  Coliformes Termotolerantes      151  58.01    \n",
       "3               LSTM  Coliformes Termotolerantes      151  28.00    \n",
       "4   Regressão Linear         Oxigênio Dissolvido      204  17.57    \n",
       "5      Random Forest         Oxigênio Dissolvido      204  19.25    \n",
       "6                MLP         Oxigênio Dissolvido      204  30.88    \n",
       "7               LSTM         Oxigênio Dissolvido      204  17.47    \n",
       "8   Regressão Linear                    Turbidez      189  34.37    \n",
       "9      Random Forest                    Turbidez      189  33.86    \n",
       "10               MLP                    Turbidez      189  58.59    \n",
       "11              LSTM                    Turbidez      189  36.43    \n",
       "12  Regressão Linear               Fósforo Total      192  14.88    \n",
       "13     Random Forest               Fósforo Total      192  15.65    \n",
       "14               MLP               Fósforo Total      192  63.24    \n",
       "15              LSTM               Fósforo Total      192  15.54    \n",
       "16  Regressão Linear                Sólido Total      205   1.53    \n",
       "17     Random Forest                Sólido Total      205   1.25    \n",
       "18               MLP                Sólido Total      205  28.27    \n",
       "19              LSTM                Sólido Total      205   2.52    \n",
       "20  Regressão Linear                 DBO (5, 20)      161   5.42    \n",
       "21     Random Forest                 DBO (5, 20)      161   3.13    \n",
       "22               MLP                 DBO (5, 20)      161  36.91    \n",
       "23              LSTM                 DBO (5, 20)      161  36.91    \n",
       "24  Regressão Linear         Temperatura da Água      195   1.74    \n",
       "25     Random Forest         Temperatura da Água      195   1.83    \n",
       "26               MLP         Temperatura da Água      195   8.45    \n",
       "27              LSTM         Temperatura da Água      195   1.71    \n",
       "28  Regressão Linear                          pH      179   1.11    \n",
       "29     Random Forest                          pH      179   1.27    \n",
       "30               MLP                          pH      179   1.09    \n",
       "31              LSTM                          pH      179   1.08    "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predição espacial\n",
    "print('predição espacial')\n",
    "\n",
    "parameters = ['coliformes', 'od', 'turbidez', 'fosforo', 'solido', 'dbo', 'temperatura', 'ph']\n",
    "df_resultado = pd.DataFrame(columns=['tecnica','parametro','amostras','mape','lag'])\n",
    "\n",
    "DFmerge = pd.DataFrame()\n",
    "for parameter in parameters:\n",
    "    # Concatenating all the parameter files\n",
    "    df = pd.read_csv('../data/CETESB/'+ parameter + '.csv',encoding='utf-8',sep=';')\n",
    "    DFmerge = pd.concat([DFmerge, df])\n",
    "    \n",
    "    \n",
    "for parameter in DFmerge['parametro'].unique():    \n",
    "    \n",
    "    DFmerged1 = DFmerge[(DFmerge['codigo_ponto'] == 'TIET02050') &\n",
    "                      (DFmerge['parametro'] == parameter)]  \n",
    "    \n",
    "    DFmerged2 = DFmerge[(DFmerge['codigo_ponto'] == 'TIET02090') &\n",
    "                      (DFmerge['parametro'] == parameter)]\n",
    "\n",
    "    \n",
    "    \n",
    "    # DFmerge.groupby([pd.Grouper(freq='1M'), 'codigo_ponto']).mean().unstack()\n",
    "    # DFmerge.groupby(['codigo_ponto', 'UGRHI'])['valor'].count().unstack()\n",
    "\n",
    "    #DFmerged1 = DFmerged1[DFmerged1['codigo_ponto'] == 'TIET02050']    \n",
    "    DFmerged1 = clean_group_data(DFmerged1, 'valor')\n",
    "    \n",
    "    #DFmerged2 = DFmerged2[DFmerged2['codigo_ponto'] == 'TIET02090']    \n",
    "    DFmerged2 = clean_group_data(DFmerged2, 'valor')\n",
    "    \n",
    "     \n",
    "    \n",
    "    DFmerged1['data_coleta'] = pd.to_datetime(DFmerged1['data_coleta'])\n",
    "    DFmerged1.index = DFmerged1['data_coleta']    \n",
    "    DFmerged1 = DFmerged1.groupby([pd.Grouper(freq='1M'), 'parametro'])['valor'].mean().unstack()\n",
    "    DFmerged1 = DFmerged1.loc['1979-01-31':]\n",
    "       \n",
    "    DFmerged2['data_coleta'] = pd.to_datetime(DFmerged2['data_coleta'])\n",
    "    DFmerged2.index = DFmerged2['data_coleta']    \n",
    "    DFmerged2 = DFmerged2.groupby([pd.Grouper(freq='1M'), 'parametro'])['valor'].mean().unstack()\n",
    "    DFmerged2 = DFmerged2.loc['1979-01-31':]\n",
    "    \n",
    "    #coloca o dataframes com o mesmo tamanho\n",
    "    #DFmerged1 = DFmerged1.iloc[0:len(DFmerged2)]\n",
    "    #if (len(DFmerged1) > len(DFmerged2)):\n",
    "        #DFmerged1 = DFmerged1.iloc[0:len(DFmerged2)]\n",
    "    #else:\n",
    "        #DFmerged2 = DFmerged2.iloc[0:len(DFmerged1)]   \n",
    "    \n",
    "    DFmerged1 = DFmerged1.interpolate(method = 'pad')\n",
    "    DFmerged2 = DFmerged2.interpolate(method = 'pad')\n",
    "    \n",
    "    DFmerged1[parameter] = tranformacao_log(DFmerged1[parameter])\n",
    "    DFmerged2[parameter] = tranformacao_log(DFmerged2[parameter])\n",
    "    \n",
    "    DFmerged3 = DFmerged1.merge(DFmerged2, how='inner', on = 'data_coleta')\n",
    "    \n",
    "    #dfnovo = DFmerged3\n",
    "    #dfnovo = dfnovo.reset_index()\n",
    "            \n",
    "    #Dividindo dataframe\n",
    "    df2050 = DFmerged3.iloc[:,0:1]\n",
    "    df2090 = DFmerged3.iloc[:,1:2]\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    #DFmerged = scaler.fit_transform(np.log(DFmerged + 0.0000001))\n",
    "#     DFmerged = scaler.fit_transform()\n",
    "\n",
    "    # #Mostra a quantidade de NaN no dataframe\n",
    "    #for field in DFmerge.columns:\n",
    "        #print(field, 'NaN:', DFmerge[field].isnull().sum())\n",
    "    #print(parameter)    \n",
    "    #print('Valor', 'NaN:', DFmerge['valor'].isnull().sum())\n",
    "    \n",
    "        \n",
    "    X  = df2050[:]\n",
    "    y  = df2090[:]\n",
    "\n",
    "\n",
    "    dataset = pd.DataFrame(np.concatenate([X,y], axis=1))\n",
    "    dataset.columns = ['X','y']\n",
    "    dataset.dropna(inplace=True)\n",
    "\n",
    "    #train_X = scaler.fit_transform(np.array(dataset['X']).reshape(len(dataset), 1))\n",
    "    #train_y = scaler.fit_transform(np.log(np.array(dataset['y']).reshape(len(dataset), 1) + 0.000000001))\n",
    "    \n",
    "    train_X = scaler.fit_transform(np.array(dataset['X']).reshape(len(dataset), 1))\n",
    "    train_y = scaler.fit_transform(np.array(dataset['y']).reshape(len(dataset), 1))\n",
    "        \n",
    "    #tamanho_treino = int(len(train_X)*0.7)\n",
    "    tamanho_teste  = int(len(train_y)*0.3)\n",
    "\n",
    "    #Desnormalização train_y(teste)\n",
    "    y_teste = scaler.inverse_transform(train_y[-tamanho_teste:])\n",
    "\n",
    "    #Regressão Linear\n",
    "    model = LinearRegression(normalize=False)\n",
    "\n",
    "    #model.fit(train_X[:-40],train_y[:-40])\n",
    "    model.fit(train_X[:-tamanho_teste],train_y[:-tamanho_teste])\n",
    "    score = model.score(train_X,train_y)\n",
    "    #     print(score)\n",
    "\n",
    "    #Dados de teste\n",
    "    #previsoes = model.predict(train_X[-40:])\n",
    "    previsoes = model.predict(train_X[-tamanho_teste:])\n",
    "    previsoes = scaler.inverse_transform(previsoes)\n",
    "\n",
    "    #df_result = resultado(df_resultado,'Regressão Linear',parameter,len(DFmerged),myutils.mean_absolute_percentage_error(train_y[-40:], previsoes))\n",
    "    df_result = resultado(df_resultado,'Regressão Linear',parameter,len(DFmerged1),\n",
    "                              myutils.mean_absolute_percentage_error(y_teste, previsoes),'')\n",
    "        \n",
    "        \n",
    "    #print(parameter, len(DFmerged),'MAPE',str(myutils.mean_absolute_percentage_error(train_y[-40:], previsoes)))\n",
    "\n",
    "    #Random Forest\n",
    "    model = RandomForestRegressor()\n",
    "\n",
    "    model.fit(train_X[:-tamanho_teste],train_y[:-tamanho_teste])\n",
    "    score = model.score(train_X,train_y)\n",
    "    #     print(score)\n",
    "\n",
    "    #Dados de teste\n",
    "    previsoes = model.predict(train_X[-tamanho_teste:])\n",
    "    previsoes = scaler.inverse_transform(previsoes.reshape(-1, 1))\n",
    "\n",
    "    df_result = resultado(df_resultado,'Random Forest',parameter,len(DFmerged1),\n",
    "                              myutils.mean_absolute_percentage_error(y_teste, previsoes),'')\n",
    "\n",
    "    #MLP\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units = 10, activation = 'relu', input_dim = train_X[:-tamanho_teste].shape[1]))\n",
    "    model.add(Dense(units = 21, activation = 'relu'))\n",
    "    model.add(Dense(units = 1, activation = 'relu'))\n",
    "    model.compile(loss = 'mean_absolute_error', optimizer = 'adam',metrics = ['mean_absolute_error'])\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', patience = 5, verbose=0)\n",
    "\n",
    "    #Treina o modelo\n",
    "    history = model.fit(train_X[:-tamanho_teste],train_y[:-tamanho_teste], validation_data = (train_X[:-tamanho_teste],train_y[:-tamanho_teste]),  batch_size = 4, epochs = 2000, callbacks=[es], verbose=0)\n",
    "\n",
    "    #Dados de teste\n",
    "    media_previsoes = []\n",
    "    for r in range(0,6):\n",
    "        previsoes = model.predict(train_X[-tamanho_teste:])\n",
    "        previsoes = scaler.inverse_transform(previsoes)\n",
    "\n",
    "        media_previsoes.append(myutils.mean_absolute_percentage_error(y_teste, previsoes))  \n",
    "\n",
    "    df_result = resultado(df_resultado,'MLP',parameter,len(DFmerged1),np.mean(media_previsoes),'')\n",
    "\n",
    "\n",
    "    #LSTM\n",
    "\n",
    "    train_X = np.reshape(train_X, (train_X.shape[0], train_X.shape[1], 1))\n",
    "    train_y = np.reshape(train_y, (train_y.shape[0], train_y.shape[1], 1))\n",
    "\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units = 10, input_shape = (train_X[:-tamanho_teste].shape[1], 1)))\n",
    "    model.add(Dense(21, activation = 'relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation = 'relu'))\n",
    "    model.compile(loss = 'mean_absolute_error', optimizer = 'adam',metrics = ['mean_absolute_error'])\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', patience = 5, verbose=0)\n",
    "\n",
    "    #Treina o modelo\n",
    "    history = model.fit(train_X[:-tamanho_teste],train_y[:-tamanho_teste],validation_data = (train_X[:-tamanho_teste],train_y[:-tamanho_teste]), batch_size = 4, epochs = 100, callbacks=[es], verbose=0)\n",
    "\n",
    "    #Dados de teste\n",
    "    media_previsoes = []\n",
    "    for r in range(0,6):\n",
    "        previsoes = model.predict(train_X[-tamanho_teste:])\n",
    "        previsoes = scaler.inverse_transform(previsoes)\n",
    "        media_previsoes.append(myutils.mean_absolute_percentage_error(y_teste, previsoes))\n",
    "\n",
    "    df_result = resultado(df_resultado,'LSTM',parameter,len(DFmerged1),np.mean(media_previsoes),'')\n",
    "\n",
    "         \n",
    "    \n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.to_csv (r'/home/anderson/Downloads/predicaoagua/src/espacial_pad_limpo_relu.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1979-01-31T00:00:00.000000000', '1979-02-28T00:00:00.000000000',\n",
       "       '1979-03-31T00:00:00.000000000', '1980-01-31T00:00:00.000000000',\n",
       "       '1980-02-29T00:00:00.000000000', '1980-04-30T00:00:00.000000000',\n",
       "       '1980-06-30T00:00:00.000000000', '1980-08-31T00:00:00.000000000',\n",
       "       '1981-01-31T00:00:00.000000000', '1981-03-31T00:00:00.000000000',\n",
       "       '1981-04-30T00:00:00.000000000', '1981-05-31T00:00:00.000000000',\n",
       "       '1982-01-31T00:00:00.000000000', '1982-03-31T00:00:00.000000000',\n",
       "       '1982-04-30T00:00:00.000000000', '1982-05-31T00:00:00.000000000',\n",
       "       '1982-06-30T00:00:00.000000000', '1983-01-31T00:00:00.000000000',\n",
       "       '1983-03-31T00:00:00.000000000', '1983-04-30T00:00:00.000000000',\n",
       "       '1983-12-31T00:00:00.000000000', '1984-01-31T00:00:00.000000000',\n",
       "       '1984-02-29T00:00:00.000000000', '1984-04-30T00:00:00.000000000',\n",
       "       '1984-05-31T00:00:00.000000000', '1984-08-31T00:00:00.000000000',\n",
       "       '1985-01-31T00:00:00.000000000', '1985-02-28T00:00:00.000000000',\n",
       "       '1985-03-31T00:00:00.000000000', '1985-05-31T00:00:00.000000000',\n",
       "       '1986-01-31T00:00:00.000000000', '1986-03-31T00:00:00.000000000',\n",
       "       '1986-07-31T00:00:00.000000000', '1987-01-31T00:00:00.000000000',\n",
       "       '1987-04-30T00:00:00.000000000', '1987-05-31T00:00:00.000000000',\n",
       "       '1988-03-31T00:00:00.000000000', '1988-06-30T00:00:00.000000000',\n",
       "       '1989-01-31T00:00:00.000000000', '1989-07-31T00:00:00.000000000',\n",
       "       '1989-08-31T00:00:00.000000000', '1989-10-31T00:00:00.000000000',\n",
       "       '1990-04-30T00:00:00.000000000', '1990-07-31T00:00:00.000000000',\n",
       "       '1990-08-31T00:00:00.000000000', '1990-11-30T00:00:00.000000000',\n",
       "       '1990-12-31T00:00:00.000000000', '1991-08-31T00:00:00.000000000',\n",
       "       '1991-09-30T00:00:00.000000000', '1991-11-30T00:00:00.000000000',\n",
       "       '1992-03-31T00:00:00.000000000', '1992-08-31T00:00:00.000000000',\n",
       "       '1992-09-30T00:00:00.000000000', '1992-11-30T00:00:00.000000000',\n",
       "       '1993-01-31T00:00:00.000000000', '1993-03-31T00:00:00.000000000',\n",
       "       '1993-05-31T00:00:00.000000000', '1993-08-31T00:00:00.000000000',\n",
       "       '1993-09-30T00:00:00.000000000', '1994-05-31T00:00:00.000000000',\n",
       "       '1994-07-31T00:00:00.000000000', '1994-09-30T00:00:00.000000000',\n",
       "       '1994-10-31T00:00:00.000000000', '1994-12-31T00:00:00.000000000',\n",
       "       '1995-03-31T00:00:00.000000000', '1995-04-30T00:00:00.000000000',\n",
       "       '1995-05-31T00:00:00.000000000', '1995-07-31T00:00:00.000000000',\n",
       "       '1995-11-30T00:00:00.000000000', '1996-01-31T00:00:00.000000000',\n",
       "       '1996-03-31T00:00:00.000000000', '1996-05-31T00:00:00.000000000',\n",
       "       '1996-07-31T00:00:00.000000000', '1996-09-30T00:00:00.000000000',\n",
       "       '1997-01-31T00:00:00.000000000', '1998-05-31T00:00:00.000000000',\n",
       "       '1998-07-31T00:00:00.000000000', '1998-09-30T00:00:00.000000000',\n",
       "       '1998-11-30T00:00:00.000000000', '1999-01-31T00:00:00.000000000',\n",
       "       '1999-05-31T00:00:00.000000000', '1999-09-30T00:00:00.000000000',\n",
       "       '2000-01-31T00:00:00.000000000', '2000-03-31T00:00:00.000000000',\n",
       "       '2000-07-31T00:00:00.000000000', '2000-09-30T00:00:00.000000000',\n",
       "       '2000-11-30T00:00:00.000000000', '2001-03-31T00:00:00.000000000',\n",
       "       '2001-05-31T00:00:00.000000000', '2001-07-31T00:00:00.000000000',\n",
       "       '2001-09-30T00:00:00.000000000', '2001-11-30T00:00:00.000000000',\n",
       "       '2002-01-31T00:00:00.000000000', '2002-03-31T00:00:00.000000000',\n",
       "       '2002-05-31T00:00:00.000000000', '2002-09-30T00:00:00.000000000',\n",
       "       '2002-11-30T00:00:00.000000000', '2003-01-31T00:00:00.000000000',\n",
       "       '2003-03-31T00:00:00.000000000', '2003-04-30T00:00:00.000000000',\n",
       "       '2003-05-31T00:00:00.000000000', '2003-09-30T00:00:00.000000000',\n",
       "       '2003-10-31T00:00:00.000000000', '2004-01-31T00:00:00.000000000',\n",
       "       '2004-03-31T00:00:00.000000000', '2004-05-31T00:00:00.000000000',\n",
       "       '2004-07-31T00:00:00.000000000', '2004-09-30T00:00:00.000000000',\n",
       "       '2004-10-31T00:00:00.000000000', '2005-01-31T00:00:00.000000000',\n",
       "       '2005-03-31T00:00:00.000000000', '2005-07-31T00:00:00.000000000',\n",
       "       '2005-09-30T00:00:00.000000000', '2005-11-30T00:00:00.000000000',\n",
       "       '2005-12-31T00:00:00.000000000', '2006-07-31T00:00:00.000000000',\n",
       "       '2006-08-31T00:00:00.000000000', '2006-10-31T00:00:00.000000000',\n",
       "       '2006-11-30T00:00:00.000000000', '2006-12-31T00:00:00.000000000',\n",
       "       '2007-07-31T00:00:00.000000000', '2007-08-31T00:00:00.000000000',\n",
       "       '2007-09-30T00:00:00.000000000', '2007-11-30T00:00:00.000000000',\n",
       "       '2008-03-31T00:00:00.000000000', '2008-04-30T00:00:00.000000000',\n",
       "       '2008-07-31T00:00:00.000000000', '2008-08-31T00:00:00.000000000',\n",
       "       '2008-11-30T00:00:00.000000000', '2009-07-31T00:00:00.000000000',\n",
       "       '2009-08-31T00:00:00.000000000', '2009-09-30T00:00:00.000000000',\n",
       "       '2009-10-31T00:00:00.000000000', '2009-11-30T00:00:00.000000000',\n",
       "       '2010-03-31T00:00:00.000000000', '2010-05-31T00:00:00.000000000',\n",
       "       '2010-07-31T00:00:00.000000000', '2010-09-30T00:00:00.000000000',\n",
       "       '2010-11-30T00:00:00.000000000', '2011-03-31T00:00:00.000000000',\n",
       "       '2011-05-31T00:00:00.000000000', '2011-06-30T00:00:00.000000000',\n",
       "       '2011-07-31T00:00:00.000000000', '2011-08-31T00:00:00.000000000',\n",
       "       '2011-11-30T00:00:00.000000000', '2012-01-31T00:00:00.000000000',\n",
       "       '2012-05-31T00:00:00.000000000', '2012-07-31T00:00:00.000000000',\n",
       "       '2012-09-30T00:00:00.000000000', '2012-11-30T00:00:00.000000000',\n",
       "       '2013-04-30T00:00:00.000000000', '2013-05-31T00:00:00.000000000',\n",
       "       '2013-06-30T00:00:00.000000000', '2013-09-30T00:00:00.000000000',\n",
       "       '2013-10-31T00:00:00.000000000', '2013-11-30T00:00:00.000000000',\n",
       "       '2014-01-31T00:00:00.000000000', '2014-05-31T00:00:00.000000000',\n",
       "       '2014-07-31T00:00:00.000000000', '2014-09-30T00:00:00.000000000',\n",
       "       '2014-11-30T00:00:00.000000000', '2014-12-31T00:00:00.000000000',\n",
       "       '2015-01-31T00:00:00.000000000', '2015-05-31T00:00:00.000000000',\n",
       "       '2015-07-31T00:00:00.000000000', '2015-09-30T00:00:00.000000000',\n",
       "       '2015-11-30T00:00:00.000000000', '2016-07-31T00:00:00.000000000',\n",
       "       '2016-09-30T00:00:00.000000000', '2016-10-31T00:00:00.000000000',\n",
       "       '2016-11-30T00:00:00.000000000', '2017-09-30T00:00:00.000000000',\n",
       "       '2017-11-30T00:00:00.000000000', '2018-05-31T00:00:00.000000000',\n",
       "       '2018-08-31T00:00:00.000000000', '2018-09-30T00:00:00.000000000',\n",
       "       '2018-10-31T00:00:00.000000000', '2018-11-30T00:00:00.000000000',\n",
       "       '2019-09-30T00:00:00.000000000'], dtype='datetime64[ns]')"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tranformacao_log(valores_parametro):\n",
    "   \n",
    "    valores_log = []\n",
    "   \n",
    "    for i in range(len(valores_parametro)):\n",
    "        #print(np.log(np.array(valores_parametro[i]) + 0.000000001))\n",
    "        valores_log.append(np.log(np.array(valores_parametro[i]) + 0.000000001))\n",
    "       \n",
    "    return valores_log\n",
    "\n",
    "parameters = ['coliformes', 'od', 'turbidez', 'fosforo', 'solido', 'dbo', 'temperatura', 'ph']\n",
    "df_resultado = pd.DataFrame(columns=['tecnica','parametro','amostras','mape','lag'])\n",
    "\n",
    "DFmerge = pd.DataFrame()\n",
    "for parameter in parameters:\n",
    "    # Concatenating all the parameter files\n",
    "    df = pd.read_csv('../data/CETESB/'+ parameter + '.csv',encoding='utf-8',sep=';')\n",
    "    DFmerge = pd.concat([DFmerge, df])\n",
    "    \n",
    "    \n",
    "for parameter in DFmerge['parametro'].unique():    \n",
    "    DFmerged = DFmerge[(DFmerge['codigo_ponto'] == 'TIET02050') &\n",
    "                      (DFmerge['parametro'] == parameter)]  \n",
    "\n",
    "    \n",
    "\n",
    "    # DFmerge.groupby([pd.Grouper(freq='1M'), 'codigo_ponto']).mean().unstack()\n",
    "    # DFmerge.groupby(['codigo_ponto', 'UGRHI'])['valor'].count().unstack()\n",
    "\n",
    "    #DFmerged = DFmerged[DFmerged['codigo_ponto'] == 'TIET02050']    \n",
    "    DFmerged = clean_group_data(DFmerged, 'valor')\n",
    "       \n",
    "    DFmerged['data_coleta'] = pd.to_datetime(DFmerged['data_coleta'])\n",
    "    DFmerged.index = DFmerged['data_coleta']    \n",
    "    DFmerged = DFmerged.groupby([pd.Grouper(freq='1M'), 'parametro'])['valor'].mean().unstack()\n",
    "    DFmerged = DFmerged.loc['1979-01-31':]\n",
    "    \n",
    "    DFmerged.interpolate(method = 'pad')\n",
    "    \n",
    "    dfnovo = DFmerged\n",
    "    dfnovo = dfnovo.reset_index()\n",
    "    \n",
    "    dfnovo = dfnovo.iloc[]\n",
    "    data = dfnovo['data_coleta'].values\n",
    "    \n",
    "      \n",
    "    scaler = MinMaxScaler()\n",
    "    DFmerged = tranformacao_log(DFmerged[parameter])\n",
    "data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predição espaço-temporal\n"
     ]
    }
   ],
   "source": [
    "#predição espaço-temporal\n",
    "print('predição espaço-temporal')\n",
    "\n",
    "parameters = ['coliformes', 'od', 'turbidez', 'fosforo', 'solido', 'dbo', 'temperatura', 'ph']\n",
    "df_resultado = pd.DataFrame(columns=['tecnica','parametro','amostras','mape','lag'])\n",
    "\n",
    "DFmerge = pd.DataFrame()\n",
    "for parameter in parameters:\n",
    "    # Concatenating all the parameter files\n",
    "    df = pd.read_csv('../data/CETESB/'+ parameter + '.csv',encoding='utf-8',sep=';')\n",
    "    DFmerge = pd.concat([DFmerge, df])\n",
    "    \n",
    "    \n",
    "for parameter in DFmerge['parametro'].unique():    \n",
    "    \n",
    "    DFmerged1 = DFmerge[(DFmerge['codigo_ponto'] == 'TIET02050') &\n",
    "                      (DFmerge['parametro'] == parameter)]  \n",
    "    \n",
    "    DFmerged2 = DFmerge[(DFmerge['codigo_ponto'] == 'TIET02090') &\n",
    "                      (DFmerge['parametro'] == parameter)]\n",
    "\n",
    "    \n",
    "    \n",
    "    # DFmerge.groupby([pd.Grouper(freq='1M'), 'codigo_ponto']).mean().unstack()\n",
    "    # DFmerge.groupby(['codigo_ponto', 'UGRHI'])['valor'].count().unstack()\n",
    "\n",
    "    #DFmerged1 = DFmerged1[DFmerged1['codigo_ponto'] == 'TIET02050']    \n",
    "    DFmerged1 = clean_group_data(DFmerged1, 'valor')\n",
    "    \n",
    "    #DFmerged2 = DFmerged2[DFmerged2['codigo_ponto'] == 'TIET02090']    \n",
    "    DFmerged2 = clean_group_data(DFmerged2, 'valor')\n",
    "    \n",
    "     \n",
    "    \n",
    "    DFmerged1['data_coleta'] = pd.to_datetime(DFmerged1['data_coleta'])\n",
    "    DFmerged1.index = DFmerged1['data_coleta']    \n",
    "    DFmerged1 = DFmerged1.groupby([pd.Grouper(freq='1M'), 'parametro'])['valor'].mean().unstack()\n",
    "    DFmerged1 = DFmerged1.loc['1979-01-31':]\n",
    "       \n",
    "    DFmerged2['data_coleta'] = pd.to_datetime(DFmerged2['data_coleta'])\n",
    "    DFmerged2.index = DFmerged2['data_coleta']    \n",
    "    DFmerged2 = DFmerged2.groupby([pd.Grouper(freq='1M'), 'parametro'])['valor'].mean().unstack()\n",
    "    DFmerged2 = DFmerged2.loc['1979-01-31':]\n",
    "    \n",
    "    #coloca o dataframes com o mesmo tamanho\n",
    "    #DFmerged1 = DFmerged1.iloc[0:len(DFmerged2)]\n",
    "    #if (len(DFmerged1) > len(DFmerged2)):\n",
    "        #DFmerged1 = DFmerged1.iloc[0:len(DFmerged2)]\n",
    "    #else:\n",
    "        #DFmerged2 = DFmerged2.iloc[0:len(DFmerged1)]   \n",
    "    \n",
    "    \n",
    "    DFmerged1 = DFmerged1.interpolate(method = 'pad')\n",
    "    DFmerged2 = DFmerged2.interpolate(method = 'pad')\n",
    "        \n",
    "    DFmerged1[parameter] = tranformacao_log(DFmerged1[parameter])\n",
    "    DFmerged2[parameter] = tranformacao_log(DFmerged2[parameter])\n",
    "    \n",
    "    DFmerged3 = DFmerged1.merge(DFmerged2, how='inner', on = 'data_coleta')\n",
    "    \n",
    "    dfnovo = DFmerged3\n",
    "    dfnovo = dfnovo.reset_index()\n",
    "        \n",
    "    # #Mostra a quantidade de NaN no dataframe\n",
    "    #for field in DFmerge.columns:\n",
    "        #print(field, 'NaN:', DFmerge[field].isnull().sum())\n",
    "    #print(parameter)    \n",
    "    #print('Valor', 'NaN:', DFmerge['valor'].isnull().sum())\n",
    "    \n",
    "    #Dividindo dataframe\n",
    "    df2050 = DFmerged3.iloc[:,0:1]\n",
    "    df2090 = DFmerged3.iloc[:,1:2]\n",
    "                            \n",
    "    train1,train_previsao1, test1 = pegar_dados_coluna_predita_train_test(df2050,0.70,0)\n",
    "    y_test1 = test1\n",
    "    \n",
    "  \n",
    "    train2,train_previsao2, test2 = pegar_dados_coluna_predita_train_test(df2090,0.70,0)\n",
    "    y_test2 = test2\n",
    "    \n",
    "    for lags in range(1,11):\n",
    "        \n",
    "        #normalização dos dados\n",
    "        scaler = MinMaxScaler()\n",
    "        train1,test1,testd1 = normalizacao(train1,test1)\n",
    "        train2,test2,testd2 = normalizacao(train2,test2)\n",
    "                \n",
    "        #Prepara os dados de treinamento\n",
    "        train_X1,train_y1 = prepara_dados(train1, lags,0)  \n",
    "        train_X_lstm1 = train_X1\n",
    "        train_y_lstm1 = train_y1\n",
    "        \n",
    "        train_X2,train_y2 = prepara_dados(train2, lags,0)  \n",
    "        train_X_lstm2 = train_X2\n",
    "        train_y_lstm2 = train_y2\n",
    "                \n",
    "        #Ajusta a dimensão de train_X \n",
    "        nsamples, nx, ny = train_X1.shape\n",
    "        train_X1 = train_X1.reshape((nsamples,nx*ny))\n",
    "        \n",
    "        nsamples, nx, ny = train_X2.shape\n",
    "        train_X2 = train_X2.reshape((nsamples,nx*ny))\n",
    "    \n",
    "        #Prepara os dados de teste\n",
    "        entradas1 = df2050[len(df2050) - len(test1) - lags:].values\n",
    "        entradas1 = scaler.fit_transform(entradas1)  \n",
    "    \n",
    "        test_X1 = []\n",
    "        for i in range(lags, lags+len(test1)):        \n",
    "            test_X1.append(entradas1[i-lags:i, 0:1])\n",
    "        test_X1 = np.array(test_X1)\n",
    "        \n",
    "        test_X_lstm1 = test_X1\n",
    "        \n",
    "        entradas2 = df2090[len(df2090) - len(test2) - lags:].values\n",
    "        entradas2 = scaler.fit_transform(entradas2)  \n",
    "    \n",
    "        test_X2 = []\n",
    "        for i in range(lags, lags+len(test2)):        \n",
    "            test_X2.append(entradas2[i-lags:i, 0:1])\n",
    "        test_X2 = np.array(test_X2)\n",
    "        \n",
    "        \n",
    "        test_X_lstm2 = test_X2\n",
    "                \n",
    "        #Ajusta a dimensão de test_X e train_X\n",
    "        nsamples, nx, ny = test_X1.shape\n",
    "        test_X1 = test_X1.reshape((nsamples,nx*ny))\n",
    "        \n",
    "        nsamples, nx, ny = test_X2.shape\n",
    "        test_X2 = test_X2.reshape((nsamples,nx*ny))\n",
    "        \n",
    "              \n",
    "        '''\n",
    "        dataset = pd.DataFrame(np.concatenate([X,y], axis=1))\n",
    "        dataset.columns = ['X','y']\n",
    "        dataset.dropna(inplace=True)\n",
    "        '''\n",
    "                        \n",
    "        #Regressão Linear\n",
    "        model = LinearRegression(normalize=False)\n",
    "\n",
    "        model.fit(train_X1,train_y1)\n",
    "        \n",
    "        #Dados de teste\n",
    "        previsoes = model.predict(test_X2).reshape(-1, 1) \n",
    "        previsoes = scaler.inverse_transform(previsoes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>parametro</th>\n",
       "      <th>pH_x</th>\n",
       "      <th>pH_y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data_coleta</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1979-01-31</th>\n",
       "      <td>1.824549</td>\n",
       "      <td>1.894617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979-02-28</th>\n",
       "      <td>1.813738</td>\n",
       "      <td>1.851076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979-03-31</th>\n",
       "      <td>1.791759</td>\n",
       "      <td>1.856298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-01-31</th>\n",
       "      <td>1.890850</td>\n",
       "      <td>1.887070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-02-29</th>\n",
       "      <td>1.840550</td>\n",
       "      <td>1.931521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-31</th>\n",
       "      <td>1.916923</td>\n",
       "      <td>1.931521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-31</th>\n",
       "      <td>1.851599</td>\n",
       "      <td>1.870263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-30</th>\n",
       "      <td>1.921325</td>\n",
       "      <td>1.887070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-30</th>\n",
       "      <td>1.863305</td>\n",
       "      <td>1.871802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-30</th>\n",
       "      <td>1.832581</td>\n",
       "      <td>1.848455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>161 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "parametro        pH_x      pH_y\n",
       "data_coleta                    \n",
       "1979-01-31   1.824549  1.894617\n",
       "1979-02-28   1.813738  1.851076\n",
       "1979-03-31   1.791759  1.856298\n",
       "1980-01-31   1.890850  1.887070\n",
       "1980-02-29   1.840550  1.931521\n",
       "...               ...       ...\n",
       "2018-05-31   1.916923  1.931521\n",
       "2018-08-31   1.851599  1.870263\n",
       "2018-09-30   1.921325  1.887070\n",
       "2018-11-30   1.863305  1.871802\n",
       "2019-09-30   1.832581  1.848455\n",
       "\n",
       "[161 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DFmerged3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/CETESB/coliformes.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-6dbe6151c46e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mparameter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# Concatenating all the parameter files\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../data/CETESB/'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mparameter\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m';'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mDFmerge\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mDFmerge\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1989\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1990\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1991\u001b[1;33m                 \u001b[0msrc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1992\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1993\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/CETESB/coliformes.csv'"
     ]
    }
   ],
   "source": [
    "parameters = ['coliformes', 'od', 'turbidez', 'fosforo', 'solido', 'dbo', 'temperatura', 'ph']\n",
    "df_resultado = pd.DataFrame(columns=['tecnica','parametro','amostras','mape','lag'])\n",
    "\n",
    "DFmerge = pd.DataFrame()\n",
    "for parameter in parameters:\n",
    "    # Concatenating all the parameter files\n",
    "    df = pd.read_csv('../data/CETESB/'+ parameter + '.csv',encoding='utf-8',sep=';')\n",
    "    DFmerge = pd.concat([DFmerge, df])\n",
    "    \n",
    "    \n",
    "for parameter in DFmerge['parametro'].unique():    \n",
    "    DFmerged = DFmerge[(DFmerge['codigo_ponto'] == 'TIET02050') &\n",
    "                      (DFmerge['parametro'] == parameter)] \n",
    "    \n",
    "DFmerged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
