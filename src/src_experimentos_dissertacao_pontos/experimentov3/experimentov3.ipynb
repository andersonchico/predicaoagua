{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b102e69578a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "import os \n",
    "#sys.path.append('../../')\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "from numpy import concatenate\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM,Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from utils import utilidades as myutils\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_group_data(dataset, parameter, range_upper = 1,range_lower = 1):\n",
    "    \n",
    "    q1, q3 = np.percentile(dataset[parameter],[25,75])\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q3 - range_upper*(1.5 * iqr)  \n",
    "    upper_bound = q3 + range_upper*(1.5 * iqr)  \n",
    "\n",
    "    # Deleting lower bound and upper bound from the dataset LinkTT2\n",
    "    dataset = dataset.loc[(dataset[parameter] >= lower_bound) & \n",
    "                                              (dataset[parameter] <= upper_bound)]\n",
    "    #dataset.interpolate()\n",
    "        \n",
    "    return dataset\n",
    "\n",
    "def resultado(df,tecnica,parameter,amostras,metrica,lag):\n",
    "    \n",
    "    df.loc[len(df)]= [tecnica,parameter,amostras,round(metrica,2),lag]\n",
    "    \n",
    "    \n",
    "    return df\n",
    "\n",
    "#Prepara o conjunto de dados em X e y, considerando a janela de visualização (lags).\n",
    "#cy = coluna que será predita\n",
    "def prepara_dados(dados,lags,cy):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(lags, len(dados)):\n",
    "        X.append(dados[i-lags:i,:])\n",
    "        y.append(dados[i, cy])\n",
    "       \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def pegar_dados_coluna_predita_train_test(trainingd,percent,index_coluna):\n",
    "    data = trainingd.iloc[:,:].values\n",
    "    train = trainingd.iloc[0:int(len(data)*percent),index_coluna:index_coluna+1].values\n",
    "    train_previsao = trainingd.iloc[0:int(len(data)*percent),index_coluna:index_coluna+1].values\n",
    "    test = trainingd.iloc[len(train):,index_coluna:index_coluna+1].values \n",
    "    '''\n",
    "    print('Nº observações:', len(data))\n",
    "    print('treino:',len(train))\n",
    "    print('teste:',len(test))\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    return train,train_previsao, test\n",
    "\n",
    "#Normalização dos dados: Normaliza os dados dentro um intervalo (0 a 1).\n",
    "def normalizacao(train,test):\n",
    "    sc = MinMaxScaler()\n",
    "    testd = test\n",
    "    train = sc.fit_transform(train)\n",
    "    test = sc.fit_transform(test)    \n",
    "    return train,test,testd\n",
    "\n",
    "def tranformacao_log(valores_parametro):\n",
    "   \n",
    "    valores_log = []\n",
    "   \n",
    "    for i in range(len(valores_parametro)):\n",
    "        #print(np.log(np.array(valores_parametro[i]) + 0.000000001))\n",
    "        valores_log.append(np.log(np.array(valores_parametro[i]) + 0.000000001))\n",
    "       \n",
    "    return valores_log\n",
    "\n",
    "def cria_dataframe_obsprevisoes(data_coleta,observado,previsao):\n",
    "    \n",
    "    dict = {'data_coleta':data_coleta,'observado': observado, 'previsao': previsao}\n",
    "    \n",
    "    dfobsprev = pd.DataFrame(dict)\n",
    "    \n",
    "    return dfobsprev\n",
    "\n",
    "def ajusta_lista(array):\n",
    "    lista = []\n",
    "    \n",
    "    for i in range(len(array)):\n",
    "        lista.append(array[i][0])\n",
    "        \n",
    "    #print('ajusta array:',lista)\n",
    "        \n",
    "    return lista\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tecnica</th>\n",
       "      <th>parametro</th>\n",
       "      <th>amostras</th>\n",
       "      <th>mape</th>\n",
       "      <th>lag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Regressão Linear</td>\n",
       "      <td>Coliformes Termotolerantes</td>\n",
       "      <td>151</td>\n",
       "      <td>55.41</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Coliformes Termotolerantes</td>\n",
       "      <td>151</td>\n",
       "      <td>55.31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP</td>\n",
       "      <td>Coliformes Termotolerantes</td>\n",
       "      <td>151</td>\n",
       "      <td>38.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>Coliformes Termotolerantes</td>\n",
       "      <td>151</td>\n",
       "      <td>40.04</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Regressão Linear</td>\n",
       "      <td>Coliformes Termotolerantes</td>\n",
       "      <td>151</td>\n",
       "      <td>41.82</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>pH</td>\n",
       "      <td>179</td>\n",
       "      <td>1.43</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>Regressão Linear</td>\n",
       "      <td>pH</td>\n",
       "      <td>179</td>\n",
       "      <td>1.66</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>pH</td>\n",
       "      <td>179</td>\n",
       "      <td>1.71</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>MLP</td>\n",
       "      <td>pH</td>\n",
       "      <td>179</td>\n",
       "      <td>1.73</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>pH</td>\n",
       "      <td>179</td>\n",
       "      <td>1.45</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>320 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              tecnica                   parametro amostras   mape lag\n",
       "0    Regressão Linear  Coliformes Termotolerantes      151  55.41   1\n",
       "1       Random Forest  Coliformes Termotolerantes      151  55.31   1\n",
       "2                 MLP  Coliformes Termotolerantes      151  38.88   1\n",
       "3                LSTM  Coliformes Termotolerantes      151  40.04   1\n",
       "4    Regressão Linear  Coliformes Termotolerantes      151  41.82   2\n",
       "..                ...                         ...      ...    ...  ..\n",
       "315              LSTM                          pH      179   1.43   9\n",
       "316  Regressão Linear                          pH      179   1.66  10\n",
       "317     Random Forest                          pH      179   1.71  10\n",
       "318               MLP                          pH      179   1.73  10\n",
       "319              LSTM                          pH      179   1.45  10\n",
       "\n",
       "[320 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = ['coliformes', 'od', 'turbidez', 'fosforo', 'solido', 'dbo', 'temperatura', 'ph']\n",
    "df_resultado = pd.DataFrame(columns=['tecnica','parametro','amostras','mape','lag'])\n",
    "\n",
    "DFmerge = pd.DataFrame()\n",
    "for parameter in parameters:\n",
    "    # Concatenating all the parameter files\n",
    "    df = pd.read_csv('../data/CETESB/'+ parameter + '.csv',encoding='utf-8',sep=';')\n",
    "    DFmerge = pd.concat([DFmerge, df])\n",
    "    \n",
    "    \n",
    "for parameter in DFmerge['parametro'].unique():    \n",
    "    DFmerged = DFmerge[(DFmerge['codigo_ponto'] == 'TIET02050') &\n",
    "                      (DFmerge['parametro'] == parameter)]  \n",
    "    \n",
    "    #print(parameter)\n",
    "     \n",
    "    DFmerged = clean_group_data(DFmerged, 'valor')\n",
    "    \n",
    "    DFmerged['data_coleta'] = pd.to_datetime(DFmerged['data_coleta'])\n",
    "    DFmerged.index = DFmerged['data_coleta']    \n",
    "    DFmerged = DFmerged.groupby([pd.Grouper(freq='1M'), 'parametro'])['valor'].mean().unstack()\n",
    "    DFmerged = DFmerged.loc['1979-01-31':]\n",
    "    \n",
    "    DFmerged = DFmerged.interpolate(method = 'pad')\n",
    "    \n",
    "    dfnovo = DFmerged\n",
    "    dfnovo = dfnovo.reset_index()\n",
    "    \n",
    "    DFmerged[parameter] = tranformacao_log(DFmerged[parameter])\n",
    "        \n",
    "    # #Mostra a quantidade de NaN no dataframe\n",
    "    #for field in DFmerge.columns:\n",
    "        #print(field, 'NaN:', DFmerge[field].isnull().sum())\n",
    "    #print(parameter)    \n",
    "    #print('Valor', 'NaN:', DFmerge['valor'].isnull().sum())\n",
    "    \n",
    "    #seleciona os dados\n",
    "    \n",
    "    train,train_previsao, test = pegar_dados_coluna_predita_train_test(DFmerged,0.70,0)\n",
    "    y_test = test\n",
    "        \n",
    "    for lags in range(1,11):\n",
    "        \n",
    "        #normalização dos dados\n",
    "        scaler = MinMaxScaler()\n",
    "        train,test,testd = normalizacao(train,test)\n",
    "                \n",
    "        #Prepara os dados de treinamento\n",
    "        train_X,train_y = prepara_dados(train, lags,0)  \n",
    "        train_X_lstm = train_X\n",
    "        train_y_lstm = train_y\n",
    "                \n",
    "        #Ajusta a dimensão de train_X \n",
    "        nsamples, nx, ny = train_X.shape\n",
    "        train_X = train_X.reshape((nsamples,nx*ny))\n",
    "    \n",
    "        #Prepara os dados de teste\n",
    "        entradas = DFmerged[len(DFmerged) - len(test) - lags:].values\n",
    "        entradas = scaler.fit_transform(entradas)  \n",
    "    \n",
    "        test_X = []\n",
    "        for i in range(lags, lags+len(test)):        \n",
    "            test_X.append(entradas[i-lags:i, 0:1])\n",
    "        test_X = np.array(test_X)\n",
    "        \n",
    "        test_X_lstm = test_X\n",
    "                \n",
    "        #Ajusta a dimensão de test_X e train_X\n",
    "        nsamples, nx, ny = test_X.shape\n",
    "        test_X = test_X.reshape((nsamples,nx*ny))\n",
    "                        \n",
    "        '''\n",
    "        dataset = pd.DataFrame(np.concatenate([X,y], axis=1))\n",
    "        dataset.columns = ['X','y']\n",
    "        dataset.dropna(inplace=True)\n",
    "        '''\n",
    "                                \n",
    "        #Regressão Linear\n",
    "        model = LinearRegression(normalize=False)\n",
    "\n",
    "        model.fit(train_X,train_y)\n",
    "        \n",
    "        #Dados de teste\n",
    "        previsoes = model.predict(test_X).reshape(-1, 1) \n",
    "        previsoes = scaler.inverse_transform(previsoes)\n",
    "        '''\n",
    "        data = dfnovo.iloc[len(train):,0:1].values \n",
    "        data = ajusta_lista(data)\n",
    "        observado = ajusta_lista(y_test)\n",
    "        previsao  = ajusta_lista(previsoes)\n",
    "        '''\n",
    "        #print('y_test:',len(y_test))\n",
    "        #print('previsoes:',len(previsoes))\n",
    "               \n",
    "        df_result = resultado(df_resultado,'Regressão Linear',parameter,len(DFmerged),\n",
    "                              myutils.mean_absolute_percentage_error(y_test[:len(previsoes)], previsoes),lags)\n",
    "        '''\n",
    "        dfprevisoes = cria_dataframe_obsprevisoes(data,observado,previsao)\n",
    "        \n",
    "        dfprevisoes.to_csv (r'/home/anderson/Downloads/predicaoagua/src/previsoes/temporal_rl_lag'+str(lags)+parameter+'.csv', index = True, header=True)\n",
    "        '''\n",
    "        #Random Forest\n",
    "        model = RandomForestRegressor()\n",
    "\n",
    "        model.fit(train_X,train_y)\n",
    "        \n",
    "        #Dados de teste\n",
    "        previsoes = model.predict(test_X).reshape(-1, 1)\n",
    "        previsoes = scaler.inverse_transform(previsoes)\n",
    "        '''\n",
    "        data = dfnovo.iloc[len(train):,0:1].values \n",
    "        data = ajusta_lista(data)\n",
    "        observado = ajusta_lista(y_test)\n",
    "        previsao  = ajusta_lista(previsoes)\n",
    "        '''\n",
    "        df_result = resultado(df_resultado,'Random Forest',parameter,len(DFmerged),\n",
    "                              myutils.mean_absolute_percentage_error(y_test[:len(previsoes)], previsoes),lags)\n",
    "        '''\n",
    "        dfprevisoes = cria_dataframe_obsprevisoes(data,observado,previsao)\n",
    "        \n",
    "        dfprevisoes.to_csv (r'/home/anderson/Downloads/predicaoagua/src/previsoes/temporal_rf_lag'+str(lags)+parameter+'.csv', index = True, header=True)\n",
    "        '''\n",
    "        \n",
    "        #MLP\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units = 10, activation = 'relu', input_dim = train_X.shape[1]))\n",
    "        model.add(Dense(units = 21, activation = 'relu'))\n",
    "        model.add(Dense(units = 1, activation = 'relu'))\n",
    "        model.compile(loss = 'mean_absolute_error', optimizer = 'adam',metrics = ['mean_absolute_error'])\n",
    "\n",
    "        es = EarlyStopping(monitor='val_loss', patience = 5, verbose=0)\n",
    "\n",
    "        #Treina o modelo\n",
    "        history = model.fit(train_X,train_y, validation_data = (train_X,train_y),  batch_size = 4, epochs = 2000, callbacks=[es], verbose=0)\n",
    "\n",
    "        #Dados de teste\n",
    "        media_previsoes = []\n",
    "        for r in range(0,6):\n",
    "            previsoes = model.predict(test_X)\n",
    "            previsoes = scaler.inverse_transform(previsoes)\n",
    "\n",
    "            media_previsoes.append(myutils.mean_absolute_percentage_error(y_test[:len(previsoes)], previsoes))  \n",
    "            \n",
    "        '''\n",
    "        data = dfnovo.iloc[len(train):,0:1].values \n",
    "        data = ajusta_lista(data)\n",
    "        observado = ajusta_lista(y_test)\n",
    "        previsao  = ajusta_lista(previsoes)\n",
    "        \n",
    "        dfprevisoes = cria_dataframe_obsprevisoes(data,observado,previsao)\n",
    "        \n",
    "        dfprevisoes.to_csv (r'/home/anderson/Downloads/predicaoagua/src/previsoes/temporal_MLP_lag'+str(lags)+parameter+'.csv', index = True, header=True)\n",
    "        '''\n",
    "\n",
    "        df_result = resultado(df_resultado,'MLP',parameter,len(DFmerged),np.mean(media_previsoes),lags)\n",
    "\n",
    "        \n",
    "        #LSTM\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(units = 10, input_shape = (train_X_lstm.shape[1], 1)))\n",
    "        model.add(Dense(21, activation = 'relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(1, activation = 'relu'))\n",
    "        model.compile(loss = 'mean_absolute_error', optimizer = 'adam',metrics = ['mean_absolute_error'])\n",
    "\n",
    "        es = EarlyStopping(monitor='val_loss', patience = 5, verbose=0)\n",
    "\n",
    "        #Treina o modelo\n",
    "        history = model.fit(train_X_lstm,train_y_lstm,validation_data = (train_X_lstm,train_y_lstm), batch_size = 4, epochs = 100, callbacks=[es], verbose=0)\n",
    "\n",
    "        #Dados de teste\n",
    "        media_previsoes = []\n",
    "        for r in range(0,6):\n",
    "            previsoes = model.predict(test_X_lstm)\n",
    "            previsoes = scaler.inverse_transform(previsoes)\n",
    "            media_previsoes.append(myutils.mean_absolute_percentage_error(y_test[:len(previsoes)], previsoes))\n",
    "        '''\n",
    "        data = dfnovo.iloc[len(train):,0:1].values \n",
    "        data = ajusta_lista(data)\n",
    "        observado = ajusta_lista(y_test)\n",
    "        previsao  = ajusta_lista(previsoes)\n",
    "        \n",
    "        dfprevisoes = cria_dataframe_obsprevisoes(data,observado,previsao)\n",
    "        \n",
    "        dfprevisoes.to_csv (r'/home/anderson/Downloads/predicaoagua/src/previsoes/temporal_LSTM_lag'+str(lags)+parameter+'.csv', index = True, header=True)\n",
    "        '''\n",
    "        df_result = resultado(df_resultado,'LSTM',parameter,len(DFmerged),np.mean(media_previsoes),lags)\n",
    "\n",
    "        #77.24\n",
    "        #60.56\n",
    "        #42.73\n",
    "    \n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.to_csv (r'/home/anderson/Downloads/predicaoagua/src/temporal_pad_lag_limpo_relu.csv', index = True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predição espaço-temporal\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tecnica</th>\n",
       "      <th>parametro</th>\n",
       "      <th>amostras</th>\n",
       "      <th>mape</th>\n",
       "      <th>lag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Regressão Linear</td>\n",
       "      <td>Coliformes Termotolerantes</td>\n",
       "      <td>137</td>\n",
       "      <td>32.85</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Coliformes Termotolerantes</td>\n",
       "      <td>137</td>\n",
       "      <td>32.98</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP</td>\n",
       "      <td>Coliformes Termotolerantes</td>\n",
       "      <td>137</td>\n",
       "      <td>29.82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>Coliformes Termotolerantes</td>\n",
       "      <td>137</td>\n",
       "      <td>21.02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Regressão Linear</td>\n",
       "      <td>Coliformes Termotolerantes</td>\n",
       "      <td>137</td>\n",
       "      <td>40.84</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>pH</td>\n",
       "      <td>161</td>\n",
       "      <td>0.87</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>Regressão Linear</td>\n",
       "      <td>pH</td>\n",
       "      <td>161</td>\n",
       "      <td>0.92</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>pH</td>\n",
       "      <td>161</td>\n",
       "      <td>0.99</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>MLP</td>\n",
       "      <td>pH</td>\n",
       "      <td>161</td>\n",
       "      <td>2.44</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>pH</td>\n",
       "      <td>161</td>\n",
       "      <td>0.88</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>320 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              tecnica                   parametro amostras   mape lag\n",
       "0    Regressão Linear  Coliformes Termotolerantes      137  32.85   1\n",
       "1       Random Forest  Coliformes Termotolerantes      137  32.98   1\n",
       "2                 MLP  Coliformes Termotolerantes      137  29.82   1\n",
       "3                LSTM  Coliformes Termotolerantes      137  21.02   1\n",
       "4    Regressão Linear  Coliformes Termotolerantes      137  40.84   2\n",
       "..                ...                         ...      ...    ...  ..\n",
       "315              LSTM                          pH      161   0.87   9\n",
       "316  Regressão Linear                          pH      161   0.92  10\n",
       "317     Random Forest                          pH      161   0.99  10\n",
       "318               MLP                          pH      161   2.44  10\n",
       "319              LSTM                          pH      161   0.88  10\n",
       "\n",
       "[320 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predição espaço-temporal\n",
    "print('predição espaço-temporal')\n",
    "\n",
    "parameters = ['coliformes', 'od', 'turbidez', 'fosforo', 'solido', 'dbo', 'temperatura', 'ph']\n",
    "df_resultado = pd.DataFrame(columns=['tecnica','parametro','amostras','mape','lag'])\n",
    "\n",
    "DFmerge = pd.DataFrame()\n",
    "for parameter in parameters:\n",
    "    # Concatenating all the parameter files\n",
    "    df = pd.read_csv('../data/CETESB/'+ parameter + '.csv',encoding='utf-8',sep=';')\n",
    "    DFmerge = pd.concat([DFmerge, df])\n",
    "    \n",
    "    \n",
    "for parameter in DFmerge['parametro'].unique():    \n",
    "    \n",
    "    DFmerged1 = DFmerge[(DFmerge['codigo_ponto'] == 'TIET02050') &\n",
    "                      (DFmerge['parametro'] == parameter)]  \n",
    "    \n",
    "    DFmerged2 = DFmerge[(DFmerge['codigo_ponto'] == 'TIET02090') &\n",
    "                      (DFmerge['parametro'] == parameter)]\n",
    "\n",
    "    \n",
    "    \n",
    "    # DFmerge.groupby([pd.Grouper(freq='1M'), 'codigo_ponto']).mean().unstack()\n",
    "    # DFmerge.groupby(['codigo_ponto', 'UGRHI'])['valor'].count().unstack()\n",
    "\n",
    "    #DFmerged1 = DFmerged1[DFmerged1['codigo_ponto'] == 'TIET02050']    \n",
    "    DFmerged1 = clean_group_data(DFmerged1, 'valor')\n",
    "    \n",
    "    #DFmerged2 = DFmerged2[DFmerged2['codigo_ponto'] == 'TIET02090']    \n",
    "    DFmerged2 = clean_group_data(DFmerged2, 'valor')\n",
    "    \n",
    "     \n",
    "    \n",
    "    DFmerged1['data_coleta'] = pd.to_datetime(DFmerged1['data_coleta'])\n",
    "    DFmerged1.index = DFmerged1['data_coleta']    \n",
    "    DFmerged1 = DFmerged1.groupby([pd.Grouper(freq='1M'), 'parametro'])['valor'].mean().unstack()\n",
    "    DFmerged1 = DFmerged1.loc['1979-01-31':]\n",
    "       \n",
    "    DFmerged2['data_coleta'] = pd.to_datetime(DFmerged2['data_coleta'])\n",
    "    DFmerged2.index = DFmerged2['data_coleta']    \n",
    "    DFmerged2 = DFmerged2.groupby([pd.Grouper(freq='1M'), 'parametro'])['valor'].mean().unstack()\n",
    "    DFmerged2 = DFmerged2.loc['1979-01-31':]\n",
    "    \n",
    "    #coloca o dataframes com o mesmo tamanho\n",
    "    #DFmerged1 = DFmerged1.iloc[0:len(DFmerged2)]\n",
    "    #if (len(DFmerged1) > len(DFmerged2)):\n",
    "        #DFmerged1 = DFmerged1.iloc[0:len(DFmerged2)]\n",
    "    #else:\n",
    "        #DFmerged2 = DFmerged2.iloc[0:len(DFmerged1)]   \n",
    "    \n",
    "    \n",
    "    DFmerged1 = DFmerged1.interpolate(method = 'pad')\n",
    "    DFmerged2 = DFmerged2.interpolate(method = 'pad')\n",
    "        \n",
    "    DFmerged1[parameter] = tranformacao_log(DFmerged1[parameter])\n",
    "    DFmerged2[parameter] = tranformacao_log(DFmerged2[parameter])\n",
    "    \n",
    "    DFmerged3 = DFmerged1.merge(DFmerged2, how='inner', on = 'data_coleta')\n",
    "    \n",
    "    dfnovo = DFmerged3\n",
    "    dfnovo = dfnovo.reset_index()\n",
    "        \n",
    "    # #Mostra a quantidade de NaN no dataframe\n",
    "    #for field in DFmerge.columns:\n",
    "        #print(field, 'NaN:', DFmerge[field].isnull().sum())\n",
    "    #print(parameter)    \n",
    "    #print('Valor', 'NaN:', DFmerge['valor'].isnull().sum())\n",
    "    \n",
    "    #Dividindo dataframe\n",
    "    df2050 = DFmerged3.iloc[:,0:1]\n",
    "    df2090 = DFmerged3.iloc[:,1:2]\n",
    "    \n",
    "    \n",
    "    #seleciona os dados\n",
    "    \n",
    "    train1,train_previsao1, test1 = pegar_dados_coluna_predita_train_test(df2050,0.70,0)\n",
    "    y_test1 = test1\n",
    "    \n",
    "    train2,train_previsao2, test2 = pegar_dados_coluna_predita_train_test(df2090,0.70,0)\n",
    "    y_test2 = test2\n",
    "        \n",
    "    for lags in range(1,11):\n",
    "        \n",
    "        #normalização dos dados\n",
    "        scaler = MinMaxScaler()\n",
    "        train1,test1,testd1 = normalizacao(train1,test1)\n",
    "        train2,test2,testd2 = normalizacao(train2,test2)\n",
    "                \n",
    "        #Prepara os dados de treinamento\n",
    "        train_X1,train_y1 = prepara_dados(train1, lags,0)  \n",
    "        train_X_lstm1 = train_X1\n",
    "        train_y_lstm1 = train_y1\n",
    "        \n",
    "        train_X2,train_y2 = prepara_dados(train2, lags,0)  \n",
    "        train_X_lstm2 = train_X2\n",
    "        train_y_lstm2 = train_y2\n",
    "                \n",
    "        #Ajusta a dimensão de train_X \n",
    "        nsamples, nx, ny = train_X1.shape\n",
    "        train_X1 = train_X1.reshape((nsamples,nx*ny))\n",
    "        \n",
    "        nsamples, nx, ny = train_X2.shape\n",
    "        train_X2 = train_X2.reshape((nsamples,nx*ny))\n",
    "    \n",
    "        #Prepara os dados de teste\n",
    "        entradas1 = df2050[len(df2050) - len(test1) - lags:].values\n",
    "        entradas1 = scaler.fit_transform(entradas1)  \n",
    "    \n",
    "        test_X1 = []\n",
    "        for i in range(lags, lags+len(test1)):        \n",
    "            test_X1.append(entradas1[i-lags:i, 0:1])\n",
    "        test_X1 = np.array(test_X1)\n",
    "        \n",
    "        test_X_lstm1 = test_X1\n",
    "        \n",
    "        entradas2 = df2090[len(df2090) - len(test2) - lags:].values\n",
    "        entradas2 = scaler.fit_transform(entradas2)  \n",
    "    \n",
    "        test_X2 = []\n",
    "        for i in range(lags, lags+len(test2)):        \n",
    "            test_X2.append(entradas2[i-lags:i, 0:1])\n",
    "        test_X2 = np.array(test_X2)\n",
    "        \n",
    "        test_X_lstm2 = test_X2\n",
    "                \n",
    "        #Ajusta a dimensão de test_X e train_X\n",
    "        nsamples, nx, ny = test_X1.shape\n",
    "        test_X1 = test_X1.reshape((nsamples,nx*ny))\n",
    "        \n",
    "        nsamples, nx, ny = test_X2.shape\n",
    "        test_X2 = test_X2.reshape((nsamples,nx*ny))\n",
    "                        \n",
    "        '''\n",
    "        dataset = pd.DataFrame(np.concatenate([X,y], axis=1))\n",
    "        dataset.columns = ['X','y']\n",
    "        dataset.dropna(inplace=True)\n",
    "        '''\n",
    "                        \n",
    "        #Regressão Linear\n",
    "        model = LinearRegression(normalize=False)\n",
    "\n",
    "        model.fit(train_X1,train_y1)\n",
    "        \n",
    "        #Dados de teste\n",
    "        previsoes = model.predict(test_X2).reshape(-1, 1) \n",
    "        previsoes = scaler.inverse_transform(previsoes)\n",
    "        '''\n",
    "        data = dfnovo.iloc[len(train2):,0:1].values \n",
    "        data = ajusta_lista(data)\n",
    "        observado = ajusta_lista(y_test2)\n",
    "        previsao  = ajusta_lista(previsoes)\n",
    "        \n",
    "        dfprevisoes = cria_dataframe_obsprevisoes(data,observado,previsao)\n",
    "        \n",
    "        dfprevisoes.to_csv (r'/home/anderson/Downloads/predicaoagua/src/previsoes/espaco_temporal_rl_lag'+str(lags)+parameter+'.csv', index = True, header=True)\n",
    "        '''\n",
    "        \n",
    "        #print('y_test:',len(y_test))\n",
    "        #print('previsoes:',len(previsoes))\n",
    "               \n",
    "        df_result = resultado(df_resultado,'Regressão Linear',parameter,len(DFmerged3),\n",
    "                              myutils.mean_absolute_percentage_error(y_test2[:len(previsoes)], previsoes),lags)\n",
    "        \n",
    "        \n",
    "        #Random Forest\n",
    "        model = RandomForestRegressor()\n",
    "\n",
    "        model.fit(train_X1,train_y1)\n",
    "        \n",
    "        #Dados de teste\n",
    "        previsoes = model.predict(test_X2).reshape(-1, 1)\n",
    "        previsoes = scaler.inverse_transform(previsoes)\n",
    "        '''\n",
    "        data = dfnovo.iloc[len(train2):,0:1].values \n",
    "        data = ajusta_lista(data)\n",
    "        observado = ajusta_lista(y_test2)\n",
    "        previsao  = ajusta_lista(previsoes)\n",
    "        \n",
    "        dfprevisoes = cria_dataframe_obsprevisoes(data,observado,previsao)\n",
    "        \n",
    "        dfprevisoes.to_csv (r'/home/anderson/Downloads/predicaoagua/src/previsoes/espaco_temporal_rf_lag'+str(lags)+parameter+'.csv', index = True, header=True)\n",
    "        '''\n",
    "\n",
    "        df_result = resultado(df_resultado,'Random Forest',parameter,len(DFmerged3),\n",
    "                              myutils.mean_absolute_percentage_error(y_test2[:len(previsoes)], previsoes),lags)\n",
    "        \n",
    "        #MLP\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units = 10, activation = 'relu', input_dim = train_X1.shape[1]))\n",
    "        model.add(Dense(units = 21, activation = 'relu'))\n",
    "        model.add(Dense(units = 1, activation = 'relu'))\n",
    "        model.compile(loss = 'mean_absolute_error', optimizer = 'adam',metrics = ['mean_absolute_error'])\n",
    "\n",
    "        es = EarlyStopping(monitor='val_loss', patience = 5, verbose=0)\n",
    "\n",
    "        #Treina o modelo\n",
    "        history = model.fit(train_X1,train_y1, validation_data = (train_X2,train_y2),  batch_size = 4, epochs = 2000, callbacks=[es], verbose=0)\n",
    "\n",
    "        #Dados de teste\n",
    "        media_previsoes = []\n",
    "        for r in range(0,6):\n",
    "            previsoes = model.predict(test_X2)\n",
    "            previsoes = scaler.inverse_transform(previsoes)\n",
    "\n",
    "            media_previsoes.append(myutils.mean_absolute_percentage_error(y_test2[:len(previsoes)], previsoes))  \n",
    "\n",
    "        '''\n",
    "        data = dfnovo.iloc[len(train2):,0:1].values \n",
    "        data = ajusta_lista(data)\n",
    "        observado = ajusta_lista(y_test2)\n",
    "        previsao  = ajusta_lista(previsoes)\n",
    "        \n",
    "        dfprevisoes = cria_dataframe_obsprevisoes(data,observado,previsao)\n",
    "        \n",
    "        dfprevisoes.to_csv (r'/home/anderson/Downloads/predicaoagua/src/previsoes/espaco_temporal_mlp_lag'+str(lags)+parameter+'.csv', index = True, header=True)\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        df_result = resultado(df_resultado,'MLP',parameter,len(DFmerged3),np.mean(media_previsoes),lags)\n",
    "\n",
    "        \n",
    "        #LSTM\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(units = 10, input_shape = (train_X_lstm1.shape[1], 1)))\n",
    "        model.add(Dense(21, activation = 'relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(1, activation = 'relu'))\n",
    "        model.compile(loss = 'mean_absolute_error', optimizer = 'adam',metrics = ['mean_absolute_error'])\n",
    "\n",
    "        es = EarlyStopping(monitor='val_loss', patience = 5, verbose=0)\n",
    "\n",
    "        #Treina o modelo\n",
    "        history = model.fit(train_X_lstm1,train_y_lstm1,validation_data = (train_X_lstm2,train_y_lstm2), batch_size = 4, epochs = 100, callbacks=[es], verbose=0)\n",
    "\n",
    "        #Dados de teste\n",
    "        media_previsoes = []\n",
    "        for r in range(0,6):\n",
    "            previsoes = model.predict(test_X_lstm2)\n",
    "            previsoes = scaler.inverse_transform(previsoes)\n",
    "            media_previsoes.append(myutils.mean_absolute_percentage_error(y_test2[:len(previsoes)], previsoes))\n",
    "\n",
    "        '''\n",
    "        data = dfnovo.iloc[len(train2):,0:1].values \n",
    "        data = ajusta_lista(data)\n",
    "        observado = ajusta_lista(y_test2)\n",
    "        previsao  = ajusta_lista(previsoes)\n",
    "        \n",
    "        dfprevisoes = cria_dataframe_obsprevisoes(data,observado,previsao)\n",
    "        \n",
    "        dfprevisoes.to_csv (r'/home/anderson/Downloads/predicaoagua/src/previsoes/espaco_temporal_lstm_lag'+str(lags)+parameter+'.csv', index = True, header=True)\n",
    "        '''\n",
    "        \n",
    "        df_result = resultado(df_resultado,'LSTM',parameter,len(DFmerged3),np.mean(media_previsoes),lags)\n",
    "\n",
    "        \n",
    "    \n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.to_csv (r'/home/anderson/Downloads/predicaoagua/src/espaco_temporal_pad_lag_limpo_relu.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predição espacial\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tecnica</th>\n",
       "      <th>parametro</th>\n",
       "      <th>amostras</th>\n",
       "      <th>mape</th>\n",
       "      <th>lag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Regressão Linear</td>\n",
       "      <td>Coliformes Termotolerantes</td>\n",
       "      <td>151</td>\n",
       "      <td>28.03</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Coliformes Termotolerantes</td>\n",
       "      <td>151</td>\n",
       "      <td>26.17</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP</td>\n",
       "      <td>Coliformes Termotolerantes</td>\n",
       "      <td>151</td>\n",
       "      <td>58.01</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>Coliformes Termotolerantes</td>\n",
       "      <td>151</td>\n",
       "      <td>28.00</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Regressão Linear</td>\n",
       "      <td>Oxigênio Dissolvido</td>\n",
       "      <td>204</td>\n",
       "      <td>17.57</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Oxigênio Dissolvido</td>\n",
       "      <td>204</td>\n",
       "      <td>19.25</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MLP</td>\n",
       "      <td>Oxigênio Dissolvido</td>\n",
       "      <td>204</td>\n",
       "      <td>30.88</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>Oxigênio Dissolvido</td>\n",
       "      <td>204</td>\n",
       "      <td>17.47</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Regressão Linear</td>\n",
       "      <td>Turbidez</td>\n",
       "      <td>189</td>\n",
       "      <td>34.37</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Turbidez</td>\n",
       "      <td>189</td>\n",
       "      <td>33.86</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>MLP</td>\n",
       "      <td>Turbidez</td>\n",
       "      <td>189</td>\n",
       "      <td>58.59</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>Turbidez</td>\n",
       "      <td>189</td>\n",
       "      <td>36.43</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Regressão Linear</td>\n",
       "      <td>Fósforo Total</td>\n",
       "      <td>192</td>\n",
       "      <td>14.88</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Fósforo Total</td>\n",
       "      <td>192</td>\n",
       "      <td>15.65</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>MLP</td>\n",
       "      <td>Fósforo Total</td>\n",
       "      <td>192</td>\n",
       "      <td>63.24</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>Fósforo Total</td>\n",
       "      <td>192</td>\n",
       "      <td>15.54</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Regressão Linear</td>\n",
       "      <td>Sólido Total</td>\n",
       "      <td>205</td>\n",
       "      <td>1.53</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Sólido Total</td>\n",
       "      <td>205</td>\n",
       "      <td>1.25</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>MLP</td>\n",
       "      <td>Sólido Total</td>\n",
       "      <td>205</td>\n",
       "      <td>28.27</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>Sólido Total</td>\n",
       "      <td>205</td>\n",
       "      <td>2.52</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Regressão Linear</td>\n",
       "      <td>DBO (5, 20)</td>\n",
       "      <td>161</td>\n",
       "      <td>5.42</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>DBO (5, 20)</td>\n",
       "      <td>161</td>\n",
       "      <td>3.13</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>MLP</td>\n",
       "      <td>DBO (5, 20)</td>\n",
       "      <td>161</td>\n",
       "      <td>36.91</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>DBO (5, 20)</td>\n",
       "      <td>161</td>\n",
       "      <td>36.91</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Regressão Linear</td>\n",
       "      <td>Temperatura da Água</td>\n",
       "      <td>195</td>\n",
       "      <td>1.74</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Temperatura da Água</td>\n",
       "      <td>195</td>\n",
       "      <td>1.83</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>MLP</td>\n",
       "      <td>Temperatura da Água</td>\n",
       "      <td>195</td>\n",
       "      <td>8.45</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>Temperatura da Água</td>\n",
       "      <td>195</td>\n",
       "      <td>1.71</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Regressão Linear</td>\n",
       "      <td>pH</td>\n",
       "      <td>179</td>\n",
       "      <td>1.11</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>pH</td>\n",
       "      <td>179</td>\n",
       "      <td>1.27</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>MLP</td>\n",
       "      <td>pH</td>\n",
       "      <td>179</td>\n",
       "      <td>1.09</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>pH</td>\n",
       "      <td>179</td>\n",
       "      <td>1.08</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tecnica                   parametro amostras   mape lag\n",
       "0   Regressão Linear  Coliformes Termotolerantes      151  28.03    \n",
       "1      Random Forest  Coliformes Termotolerantes      151  26.17    \n",
       "2                MLP  Coliformes Termotolerantes      151  58.01    \n",
       "3               LSTM  Coliformes Termotolerantes      151  28.00    \n",
       "4   Regressão Linear         Oxigênio Dissolvido      204  17.57    \n",
       "5      Random Forest         Oxigênio Dissolvido      204  19.25    \n",
       "6                MLP         Oxigênio Dissolvido      204  30.88    \n",
       "7               LSTM         Oxigênio Dissolvido      204  17.47    \n",
       "8   Regressão Linear                    Turbidez      189  34.37    \n",
       "9      Random Forest                    Turbidez      189  33.86    \n",
       "10               MLP                    Turbidez      189  58.59    \n",
       "11              LSTM                    Turbidez      189  36.43    \n",
       "12  Regressão Linear               Fósforo Total      192  14.88    \n",
       "13     Random Forest               Fósforo Total      192  15.65    \n",
       "14               MLP               Fósforo Total      192  63.24    \n",
       "15              LSTM               Fósforo Total      192  15.54    \n",
       "16  Regressão Linear                Sólido Total      205   1.53    \n",
       "17     Random Forest                Sólido Total      205   1.25    \n",
       "18               MLP                Sólido Total      205  28.27    \n",
       "19              LSTM                Sólido Total      205   2.52    \n",
       "20  Regressão Linear                 DBO (5, 20)      161   5.42    \n",
       "21     Random Forest                 DBO (5, 20)      161   3.13    \n",
       "22               MLP                 DBO (5, 20)      161  36.91    \n",
       "23              LSTM                 DBO (5, 20)      161  36.91    \n",
       "24  Regressão Linear         Temperatura da Água      195   1.74    \n",
       "25     Random Forest         Temperatura da Água      195   1.83    \n",
       "26               MLP         Temperatura da Água      195   8.45    \n",
       "27              LSTM         Temperatura da Água      195   1.71    \n",
       "28  Regressão Linear                          pH      179   1.11    \n",
       "29     Random Forest                          pH      179   1.27    \n",
       "30               MLP                          pH      179   1.09    \n",
       "31              LSTM                          pH      179   1.08    "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predição espacial\n",
    "print('predição espacial')\n",
    "\n",
    "parameters = ['coliformes', 'od', 'turbidez', 'fosforo', 'solido', 'dbo', 'temperatura', 'ph']\n",
    "df_resultado = pd.DataFrame(columns=['tecnica','parametro','amostras','mape','lag'])\n",
    "\n",
    "DFmerge = pd.DataFrame()\n",
    "for parameter in parameters:\n",
    "    # Concatenating all the parameter files\n",
    "    df = pd.read_csv('../data/CETESB/'+ parameter + '.csv',encoding='utf-8',sep=';')\n",
    "    DFmerge = pd.concat([DFmerge, df])\n",
    "    \n",
    "    \n",
    "for parameter in DFmerge['parametro'].unique():    \n",
    "    \n",
    "    DFmerged1 = DFmerge[(DFmerge['codigo_ponto'] == 'TIET02050') &\n",
    "                      (DFmerge['parametro'] == parameter)]  \n",
    "    \n",
    "    DFmerged2 = DFmerge[(DFmerge['codigo_ponto'] == 'TIET02090') &\n",
    "                      (DFmerge['parametro'] == parameter)]\n",
    "\n",
    "    \n",
    "    \n",
    "    # DFmerge.groupby([pd.Grouper(freq='1M'), 'codigo_ponto']).mean().unstack()\n",
    "    # DFmerge.groupby(['codigo_ponto', 'UGRHI'])['valor'].count().unstack()\n",
    "\n",
    "    #DFmerged1 = DFmerged1[DFmerged1['codigo_ponto'] == 'TIET02050']    \n",
    "    DFmerged1 = clean_group_data(DFmerged1, 'valor')\n",
    "    \n",
    "    #DFmerged2 = DFmerged2[DFmerged2['codigo_ponto'] == 'TIET02090']    \n",
    "    DFmerged2 = clean_group_data(DFmerged2, 'valor')\n",
    "    \n",
    "     \n",
    "    \n",
    "    DFmerged1['data_coleta'] = pd.to_datetime(DFmerged1['data_coleta'])\n",
    "    DFmerged1.index = DFmerged1['data_coleta']    \n",
    "    DFmerged1 = DFmerged1.groupby([pd.Grouper(freq='1M'), 'parametro'])['valor'].mean().unstack()\n",
    "    DFmerged1 = DFmerged1.loc['1979-01-31':]\n",
    "       \n",
    "    DFmerged2['data_coleta'] = pd.to_datetime(DFmerged2['data_coleta'])\n",
    "    DFmerged2.index = DFmerged2['data_coleta']    \n",
    "    DFmerged2 = DFmerged2.groupby([pd.Grouper(freq='1M'), 'parametro'])['valor'].mean().unstack()\n",
    "    DFmerged2 = DFmerged2.loc['1979-01-31':]\n",
    "    \n",
    "    #coloca o dataframes com o mesmo tamanho\n",
    "    #DFmerged1 = DFmerged1.iloc[0:len(DFmerged2)]\n",
    "    #if (len(DFmerged1) > len(DFmerged2)):\n",
    "        #DFmerged1 = DFmerged1.iloc[0:len(DFmerged2)]\n",
    "    #else:\n",
    "        #DFmerged2 = DFmerged2.iloc[0:len(DFmerged1)]   \n",
    "    \n",
    "    DFmerged1 = DFmerged1.interpolate(method = 'pad')\n",
    "    DFmerged2 = DFmerged2.interpolate(method = 'pad')\n",
    "    \n",
    "    DFmerged1[parameter] = tranformacao_log(DFmerged1[parameter])\n",
    "    DFmerged2[parameter] = tranformacao_log(DFmerged2[parameter])\n",
    "    \n",
    "    DFmerged3 = DFmerged1.merge(DFmerged2, how='inner', on = 'data_coleta')\n",
    "    \n",
    "    #dfnovo = DFmerged3\n",
    "    #dfnovo = dfnovo.reset_index()\n",
    "            \n",
    "    #Dividindo dataframe\n",
    "    df2050 = DFmerged3.iloc[:,0:1]\n",
    "    df2090 = DFmerged3.iloc[:,1:2]\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    #DFmerged = scaler.fit_transform(np.log(DFmerged + 0.0000001))\n",
    "#     DFmerged = scaler.fit_transform()\n",
    "\n",
    "    # #Mostra a quantidade de NaN no dataframe\n",
    "    #for field in DFmerge.columns:\n",
    "        #print(field, 'NaN:', DFmerge[field].isnull().sum())\n",
    "    #print(parameter)    \n",
    "    #print('Valor', 'NaN:', DFmerge['valor'].isnull().sum())\n",
    "    \n",
    "        \n",
    "    X  = df2050[:]\n",
    "    y  = df2090[:]\n",
    "\n",
    "\n",
    "    dataset = pd.DataFrame(np.concatenate([X,y], axis=1))\n",
    "    dataset.columns = ['X','y']\n",
    "    dataset.dropna(inplace=True)\n",
    "\n",
    "    #train_X = scaler.fit_transform(np.array(dataset['X']).reshape(len(dataset), 1))\n",
    "    #train_y = scaler.fit_transform(np.log(np.array(dataset['y']).reshape(len(dataset), 1) + 0.000000001))\n",
    "    \n",
    "    train_X = scaler.fit_transform(np.array(dataset['X']).reshape(len(dataset), 1))\n",
    "    train_y = scaler.fit_transform(np.array(dataset['y']).reshape(len(dataset), 1))\n",
    "        \n",
    "    #tamanho_treino = int(len(train_X)*0.7)\n",
    "    tamanho_teste  = int(len(train_y)*0.3)\n",
    "\n",
    "    #Desnormalização train_y(teste)\n",
    "    y_teste = scaler.inverse_transform(train_y[-tamanho_teste:])\n",
    "\n",
    "    #Regressão Linear\n",
    "    model = LinearRegression(normalize=False)\n",
    "\n",
    "    #model.fit(train_X[:-40],train_y[:-40])\n",
    "    model.fit(train_X[:-tamanho_teste],train_y[:-tamanho_teste])\n",
    "    score = model.score(train_X,train_y)\n",
    "    #     print(score)\n",
    "\n",
    "    #Dados de teste\n",
    "    #previsoes = model.predict(train_X[-40:])\n",
    "    previsoes = model.predict(train_X[-tamanho_teste:])\n",
    "    previsoes = scaler.inverse_transform(previsoes)\n",
    "\n",
    "    #df_result = resultado(df_resultado,'Regressão Linear',parameter,len(DFmerged),myutils.mean_absolute_percentage_error(train_y[-40:], previsoes))\n",
    "    df_result = resultado(df_resultado,'Regressão Linear',parameter,len(DFmerged1),\n",
    "                              myutils.mean_absolute_percentage_error(y_teste, previsoes),'')\n",
    "        \n",
    "        \n",
    "    #print(parameter, len(DFmerged),'MAPE',str(myutils.mean_absolute_percentage_error(train_y[-40:], previsoes)))\n",
    "\n",
    "    #Random Forest\n",
    "    model = RandomForestRegressor()\n",
    "\n",
    "    model.fit(train_X[:-tamanho_teste],train_y[:-tamanho_teste])\n",
    "    score = model.score(train_X,train_y)\n",
    "    #     print(score)\n",
    "\n",
    "    #Dados de teste\n",
    "    previsoes = model.predict(train_X[-tamanho_teste:])\n",
    "    previsoes = scaler.inverse_transform(previsoes.reshape(-1, 1))\n",
    "\n",
    "    df_result = resultado(df_resultado,'Random Forest',parameter,len(DFmerged1),\n",
    "                              myutils.mean_absolute_percentage_error(y_teste, previsoes),'')\n",
    "\n",
    "    #MLP\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units = 10, activation = 'relu', input_dim = train_X[:-tamanho_teste].shape[1]))\n",
    "    model.add(Dense(units = 21, activation = 'relu'))\n",
    "    model.add(Dense(units = 1, activation = 'relu'))\n",
    "    model.compile(loss = 'mean_absolute_error', optimizer = 'adam',metrics = ['mean_absolute_error'])\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', patience = 5, verbose=0)\n",
    "\n",
    "    #Treina o modelo\n",
    "    history = model.fit(train_X[:-tamanho_teste],train_y[:-tamanho_teste], validation_data = (train_X[:-tamanho_teste],train_y[:-tamanho_teste]),  batch_size = 4, epochs = 2000, callbacks=[es], verbose=0)\n",
    "\n",
    "    #Dados de teste\n",
    "    media_previsoes = []\n",
    "    for r in range(0,6):\n",
    "        previsoes = model.predict(train_X[-tamanho_teste:])\n",
    "        previsoes = scaler.inverse_transform(previsoes)\n",
    "\n",
    "        media_previsoes.append(myutils.mean_absolute_percentage_error(y_teste, previsoes))  \n",
    "\n",
    "    df_result = resultado(df_resultado,'MLP',parameter,len(DFmerged1),np.mean(media_previsoes),'')\n",
    "\n",
    "\n",
    "    #LSTM\n",
    "\n",
    "    train_X = np.reshape(train_X, (train_X.shape[0], train_X.shape[1], 1))\n",
    "    train_y = np.reshape(train_y, (train_y.shape[0], train_y.shape[1], 1))\n",
    "\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units = 10, input_shape = (train_X[:-tamanho_teste].shape[1], 1)))\n",
    "    model.add(Dense(21, activation = 'relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation = 'relu'))\n",
    "    model.compile(loss = 'mean_absolute_error', optimizer = 'adam',metrics = ['mean_absolute_error'])\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', patience = 5, verbose=0)\n",
    "\n",
    "    #Treina o modelo\n",
    "    history = model.fit(train_X[:-tamanho_teste],train_y[:-tamanho_teste],validation_data = (train_X[:-tamanho_teste],train_y[:-tamanho_teste]), batch_size = 4, epochs = 100, callbacks=[es], verbose=0)\n",
    "\n",
    "    #Dados de teste\n",
    "    media_previsoes = []\n",
    "    for r in range(0,6):\n",
    "        previsoes = model.predict(train_X[-tamanho_teste:])\n",
    "        previsoes = scaler.inverse_transform(previsoes)\n",
    "        media_previsoes.append(myutils.mean_absolute_percentage_error(y_teste, previsoes))\n",
    "\n",
    "    df_result = resultado(df_resultado,'LSTM',parameter,len(DFmerged1),np.mean(media_previsoes),'')\n",
    "\n",
    "         \n",
    "    \n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.to_csv (r'/home/anderson/Downloads/predicaoagua/src/espacial_pad_limpo_relu.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1979-01-31T00:00:00.000000000', '1979-02-28T00:00:00.000000000',\n",
       "       '1979-03-31T00:00:00.000000000', '1980-01-31T00:00:00.000000000',\n",
       "       '1980-02-29T00:00:00.000000000', '1980-04-30T00:00:00.000000000',\n",
       "       '1980-06-30T00:00:00.000000000', '1980-08-31T00:00:00.000000000',\n",
       "       '1981-01-31T00:00:00.000000000', '1981-03-31T00:00:00.000000000',\n",
       "       '1981-04-30T00:00:00.000000000', '1981-05-31T00:00:00.000000000',\n",
       "       '1982-01-31T00:00:00.000000000', '1982-03-31T00:00:00.000000000',\n",
       "       '1982-04-30T00:00:00.000000000', '1982-05-31T00:00:00.000000000',\n",
       "       '1982-06-30T00:00:00.000000000', '1983-01-31T00:00:00.000000000',\n",
       "       '1983-03-31T00:00:00.000000000', '1983-04-30T00:00:00.000000000',\n",
       "       '1983-12-31T00:00:00.000000000', '1984-01-31T00:00:00.000000000',\n",
       "       '1984-02-29T00:00:00.000000000', '1984-04-30T00:00:00.000000000',\n",
       "       '1984-05-31T00:00:00.000000000', '1984-08-31T00:00:00.000000000',\n",
       "       '1985-01-31T00:00:00.000000000', '1985-02-28T00:00:00.000000000',\n",
       "       '1985-03-31T00:00:00.000000000', '1985-05-31T00:00:00.000000000',\n",
       "       '1986-01-31T00:00:00.000000000', '1986-03-31T00:00:00.000000000',\n",
       "       '1986-07-31T00:00:00.000000000', '1987-01-31T00:00:00.000000000',\n",
       "       '1987-04-30T00:00:00.000000000', '1987-05-31T00:00:00.000000000',\n",
       "       '1988-03-31T00:00:00.000000000', '1988-06-30T00:00:00.000000000',\n",
       "       '1989-01-31T00:00:00.000000000', '1989-07-31T00:00:00.000000000',\n",
       "       '1989-08-31T00:00:00.000000000', '1989-10-31T00:00:00.000000000',\n",
       "       '1990-04-30T00:00:00.000000000', '1990-07-31T00:00:00.000000000',\n",
       "       '1990-08-31T00:00:00.000000000', '1990-11-30T00:00:00.000000000',\n",
       "       '1990-12-31T00:00:00.000000000', '1991-08-31T00:00:00.000000000',\n",
       "       '1991-09-30T00:00:00.000000000', '1991-11-30T00:00:00.000000000',\n",
       "       '1992-03-31T00:00:00.000000000', '1992-08-31T00:00:00.000000000',\n",
       "       '1992-09-30T00:00:00.000000000', '1992-11-30T00:00:00.000000000',\n",
       "       '1993-01-31T00:00:00.000000000', '1993-03-31T00:00:00.000000000',\n",
       "       '1993-05-31T00:00:00.000000000', '1993-08-31T00:00:00.000000000',\n",
       "       '1993-09-30T00:00:00.000000000', '1994-05-31T00:00:00.000000000',\n",
       "       '1994-07-31T00:00:00.000000000', '1994-09-30T00:00:00.000000000',\n",
       "       '1994-10-31T00:00:00.000000000', '1994-12-31T00:00:00.000000000',\n",
       "       '1995-03-31T00:00:00.000000000', '1995-04-30T00:00:00.000000000',\n",
       "       '1995-05-31T00:00:00.000000000', '1995-07-31T00:00:00.000000000',\n",
       "       '1995-11-30T00:00:00.000000000', '1996-01-31T00:00:00.000000000',\n",
       "       '1996-03-31T00:00:00.000000000', '1996-05-31T00:00:00.000000000',\n",
       "       '1996-07-31T00:00:00.000000000', '1996-09-30T00:00:00.000000000',\n",
       "       '1997-01-31T00:00:00.000000000', '1998-05-31T00:00:00.000000000',\n",
       "       '1998-07-31T00:00:00.000000000', '1998-09-30T00:00:00.000000000',\n",
       "       '1998-11-30T00:00:00.000000000', '1999-01-31T00:00:00.000000000',\n",
       "       '1999-05-31T00:00:00.000000000', '1999-09-30T00:00:00.000000000',\n",
       "       '2000-01-31T00:00:00.000000000', '2000-03-31T00:00:00.000000000',\n",
       "       '2000-07-31T00:00:00.000000000', '2000-09-30T00:00:00.000000000',\n",
       "       '2000-11-30T00:00:00.000000000', '2001-03-31T00:00:00.000000000',\n",
       "       '2001-05-31T00:00:00.000000000', '2001-07-31T00:00:00.000000000',\n",
       "       '2001-09-30T00:00:00.000000000', '2001-11-30T00:00:00.000000000',\n",
       "       '2002-01-31T00:00:00.000000000', '2002-03-31T00:00:00.000000000',\n",
       "       '2002-05-31T00:00:00.000000000', '2002-09-30T00:00:00.000000000',\n",
       "       '2002-11-30T00:00:00.000000000', '2003-01-31T00:00:00.000000000',\n",
       "       '2003-03-31T00:00:00.000000000', '2003-04-30T00:00:00.000000000',\n",
       "       '2003-05-31T00:00:00.000000000', '2003-09-30T00:00:00.000000000',\n",
       "       '2003-10-31T00:00:00.000000000', '2004-01-31T00:00:00.000000000',\n",
       "       '2004-03-31T00:00:00.000000000', '2004-05-31T00:00:00.000000000',\n",
       "       '2004-07-31T00:00:00.000000000', '2004-09-30T00:00:00.000000000',\n",
       "       '2004-10-31T00:00:00.000000000', '2005-01-31T00:00:00.000000000',\n",
       "       '2005-03-31T00:00:00.000000000', '2005-07-31T00:00:00.000000000',\n",
       "       '2005-09-30T00:00:00.000000000', '2005-11-30T00:00:00.000000000',\n",
       "       '2005-12-31T00:00:00.000000000', '2006-07-31T00:00:00.000000000',\n",
       "       '2006-08-31T00:00:00.000000000', '2006-10-31T00:00:00.000000000',\n",
       "       '2006-11-30T00:00:00.000000000', '2006-12-31T00:00:00.000000000',\n",
       "       '2007-07-31T00:00:00.000000000', '2007-08-31T00:00:00.000000000',\n",
       "       '2007-09-30T00:00:00.000000000', '2007-11-30T00:00:00.000000000',\n",
       "       '2008-03-31T00:00:00.000000000', '2008-04-30T00:00:00.000000000',\n",
       "       '2008-07-31T00:00:00.000000000', '2008-08-31T00:00:00.000000000',\n",
       "       '2008-11-30T00:00:00.000000000', '2009-07-31T00:00:00.000000000',\n",
       "       '2009-08-31T00:00:00.000000000', '2009-09-30T00:00:00.000000000',\n",
       "       '2009-10-31T00:00:00.000000000', '2009-11-30T00:00:00.000000000',\n",
       "       '2010-03-31T00:00:00.000000000', '2010-05-31T00:00:00.000000000',\n",
       "       '2010-07-31T00:00:00.000000000', '2010-09-30T00:00:00.000000000',\n",
       "       '2010-11-30T00:00:00.000000000', '2011-03-31T00:00:00.000000000',\n",
       "       '2011-05-31T00:00:00.000000000', '2011-06-30T00:00:00.000000000',\n",
       "       '2011-07-31T00:00:00.000000000', '2011-08-31T00:00:00.000000000',\n",
       "       '2011-11-30T00:00:00.000000000', '2012-01-31T00:00:00.000000000',\n",
       "       '2012-05-31T00:00:00.000000000', '2012-07-31T00:00:00.000000000',\n",
       "       '2012-09-30T00:00:00.000000000', '2012-11-30T00:00:00.000000000',\n",
       "       '2013-04-30T00:00:00.000000000', '2013-05-31T00:00:00.000000000',\n",
       "       '2013-06-30T00:00:00.000000000', '2013-09-30T00:00:00.000000000',\n",
       "       '2013-10-31T00:00:00.000000000', '2013-11-30T00:00:00.000000000',\n",
       "       '2014-01-31T00:00:00.000000000', '2014-05-31T00:00:00.000000000',\n",
       "       '2014-07-31T00:00:00.000000000', '2014-09-30T00:00:00.000000000',\n",
       "       '2014-11-30T00:00:00.000000000', '2014-12-31T00:00:00.000000000',\n",
       "       '2015-01-31T00:00:00.000000000', '2015-05-31T00:00:00.000000000',\n",
       "       '2015-07-31T00:00:00.000000000', '2015-09-30T00:00:00.000000000',\n",
       "       '2015-11-30T00:00:00.000000000', '2016-07-31T00:00:00.000000000',\n",
       "       '2016-09-30T00:00:00.000000000', '2016-10-31T00:00:00.000000000',\n",
       "       '2016-11-30T00:00:00.000000000', '2017-09-30T00:00:00.000000000',\n",
       "       '2017-11-30T00:00:00.000000000', '2018-05-31T00:00:00.000000000',\n",
       "       '2018-08-31T00:00:00.000000000', '2018-09-30T00:00:00.000000000',\n",
       "       '2018-10-31T00:00:00.000000000', '2018-11-30T00:00:00.000000000',\n",
       "       '2019-09-30T00:00:00.000000000'], dtype='datetime64[ns]')"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tranformacao_log(valores_parametro):\n",
    "   \n",
    "    valores_log = []\n",
    "   \n",
    "    for i in range(len(valores_parametro)):\n",
    "        #print(np.log(np.array(valores_parametro[i]) + 0.000000001))\n",
    "        valores_log.append(np.log(np.array(valores_parametro[i]) + 0.000000001))\n",
    "       \n",
    "    return valores_log\n",
    "\n",
    "parameters = ['coliformes', 'od', 'turbidez', 'fosforo', 'solido', 'dbo', 'temperatura', 'ph']\n",
    "df_resultado = pd.DataFrame(columns=['tecnica','parametro','amostras','mape','lag'])\n",
    "\n",
    "DFmerge = pd.DataFrame()\n",
    "for parameter in parameters:\n",
    "    # Concatenating all the parameter files\n",
    "    df = pd.read_csv('../data/CETESB/'+ parameter + '.csv',encoding='utf-8',sep=';')\n",
    "    DFmerge = pd.concat([DFmerge, df])\n",
    "    \n",
    "    \n",
    "for parameter in DFmerge['parametro'].unique():    \n",
    "    DFmerged = DFmerge[(DFmerge['codigo_ponto'] == 'TIET02050') &\n",
    "                      (DFmerge['parametro'] == parameter)]  \n",
    "\n",
    "    \n",
    "\n",
    "    # DFmerge.groupby([pd.Grouper(freq='1M'), 'codigo_ponto']).mean().unstack()\n",
    "    # DFmerge.groupby(['codigo_ponto', 'UGRHI'])['valor'].count().unstack()\n",
    "\n",
    "    #DFmerged = DFmerged[DFmerged['codigo_ponto'] == 'TIET02050']    \n",
    "    DFmerged = clean_group_data(DFmerged, 'valor')\n",
    "       \n",
    "    DFmerged['data_coleta'] = pd.to_datetime(DFmerged['data_coleta'])\n",
    "    DFmerged.index = DFmerged['data_coleta']    \n",
    "    DFmerged = DFmerged.groupby([pd.Grouper(freq='1M'), 'parametro'])['valor'].mean().unstack()\n",
    "    DFmerged = DFmerged.loc['1979-01-31':]\n",
    "    \n",
    "    DFmerged.interpolate(method = 'pad')\n",
    "    \n",
    "    dfnovo = DFmerged\n",
    "    dfnovo = dfnovo.reset_index()\n",
    "    \n",
    "    dfnovo = dfnovo.iloc[]\n",
    "    data = dfnovo['data_coleta'].values\n",
    "    \n",
    "      \n",
    "    scaler = MinMaxScaler()\n",
    "    DFmerged = tranformacao_log(DFmerged[parameter])\n",
    "data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predição espaço-temporal\n"
     ]
    }
   ],
   "source": [
    "#predição espaço-temporal\n",
    "print('predição espaço-temporal')\n",
    "\n",
    "parameters = ['coliformes', 'od', 'turbidez', 'fosforo', 'solido', 'dbo', 'temperatura', 'ph']\n",
    "df_resultado = pd.DataFrame(columns=['tecnica','parametro','amostras','mape','lag'])\n",
    "\n",
    "DFmerge = pd.DataFrame()\n",
    "for parameter in parameters:\n",
    "    # Concatenating all the parameter files\n",
    "    df = pd.read_csv('../data/CETESB/'+ parameter + '.csv',encoding='utf-8',sep=';')\n",
    "    DFmerge = pd.concat([DFmerge, df])\n",
    "    \n",
    "    \n",
    "for parameter in DFmerge['parametro'].unique():    \n",
    "    \n",
    "    DFmerged1 = DFmerge[(DFmerge['codigo_ponto'] == 'TIET02050') &\n",
    "                      (DFmerge['parametro'] == parameter)]  \n",
    "    \n",
    "    DFmerged2 = DFmerge[(DFmerge['codigo_ponto'] == 'TIET02090') &\n",
    "                      (DFmerge['parametro'] == parameter)]\n",
    "\n",
    "    \n",
    "    \n",
    "    # DFmerge.groupby([pd.Grouper(freq='1M'), 'codigo_ponto']).mean().unstack()\n",
    "    # DFmerge.groupby(['codigo_ponto', 'UGRHI'])['valor'].count().unstack()\n",
    "\n",
    "    #DFmerged1 = DFmerged1[DFmerged1['codigo_ponto'] == 'TIET02050']    \n",
    "    DFmerged1 = clean_group_data(DFmerged1, 'valor')\n",
    "    \n",
    "    #DFmerged2 = DFmerged2[DFmerged2['codigo_ponto'] == 'TIET02090']    \n",
    "    DFmerged2 = clean_group_data(DFmerged2, 'valor')\n",
    "    \n",
    "     \n",
    "    \n",
    "    DFmerged1['data_coleta'] = pd.to_datetime(DFmerged1['data_coleta'])\n",
    "    DFmerged1.index = DFmerged1['data_coleta']    \n",
    "    DFmerged1 = DFmerged1.groupby([pd.Grouper(freq='1M'), 'parametro'])['valor'].mean().unstack()\n",
    "    DFmerged1 = DFmerged1.loc['1979-01-31':]\n",
    "       \n",
    "    DFmerged2['data_coleta'] = pd.to_datetime(DFmerged2['data_coleta'])\n",
    "    DFmerged2.index = DFmerged2['data_coleta']    \n",
    "    DFmerged2 = DFmerged2.groupby([pd.Grouper(freq='1M'), 'parametro'])['valor'].mean().unstack()\n",
    "    DFmerged2 = DFmerged2.loc['1979-01-31':]\n",
    "    \n",
    "    #coloca o dataframes com o mesmo tamanho\n",
    "    #DFmerged1 = DFmerged1.iloc[0:len(DFmerged2)]\n",
    "    #if (len(DFmerged1) > len(DFmerged2)):\n",
    "        #DFmerged1 = DFmerged1.iloc[0:len(DFmerged2)]\n",
    "    #else:\n",
    "        #DFmerged2 = DFmerged2.iloc[0:len(DFmerged1)]   \n",
    "    \n",
    "    \n",
    "    DFmerged1 = DFmerged1.interpolate(method = 'pad')\n",
    "    DFmerged2 = DFmerged2.interpolate(method = 'pad')\n",
    "        \n",
    "    DFmerged1[parameter] = tranformacao_log(DFmerged1[parameter])\n",
    "    DFmerged2[parameter] = tranformacao_log(DFmerged2[parameter])\n",
    "    \n",
    "    DFmerged3 = DFmerged1.merge(DFmerged2, how='inner', on = 'data_coleta')\n",
    "    \n",
    "    dfnovo = DFmerged3\n",
    "    dfnovo = dfnovo.reset_index()\n",
    "        \n",
    "    # #Mostra a quantidade de NaN no dataframe\n",
    "    #for field in DFmerge.columns:\n",
    "        #print(field, 'NaN:', DFmerge[field].isnull().sum())\n",
    "    #print(parameter)    \n",
    "    #print('Valor', 'NaN:', DFmerge['valor'].isnull().sum())\n",
    "    \n",
    "    #Dividindo dataframe\n",
    "    df2050 = DFmerged3.iloc[:,0:1]\n",
    "    df2090 = DFmerged3.iloc[:,1:2]\n",
    "                            \n",
    "    train1,train_previsao1, test1 = pegar_dados_coluna_predita_train_test(df2050,0.70,0)\n",
    "    y_test1 = test1\n",
    "    \n",
    "  \n",
    "    train2,train_previsao2, test2 = pegar_dados_coluna_predita_train_test(df2090,0.70,0)\n",
    "    y_test2 = test2\n",
    "    \n",
    "    for lags in range(1,11):\n",
    "        \n",
    "        #normalização dos dados\n",
    "        scaler = MinMaxScaler()\n",
    "        train1,test1,testd1 = normalizacao(train1,test1)\n",
    "        train2,test2,testd2 = normalizacao(train2,test2)\n",
    "                \n",
    "        #Prepara os dados de treinamento\n",
    "        train_X1,train_y1 = prepara_dados(train1, lags,0)  \n",
    "        train_X_lstm1 = train_X1\n",
    "        train_y_lstm1 = train_y1\n",
    "        \n",
    "        train_X2,train_y2 = prepara_dados(train2, lags,0)  \n",
    "        train_X_lstm2 = train_X2\n",
    "        train_y_lstm2 = train_y2\n",
    "                \n",
    "        #Ajusta a dimensão de train_X \n",
    "        nsamples, nx, ny = train_X1.shape\n",
    "        train_X1 = train_X1.reshape((nsamples,nx*ny))\n",
    "        \n",
    "        nsamples, nx, ny = train_X2.shape\n",
    "        train_X2 = train_X2.reshape((nsamples,nx*ny))\n",
    "    \n",
    "        #Prepara os dados de teste\n",
    "        entradas1 = df2050[len(df2050) - len(test1) - lags:].values\n",
    "        entradas1 = scaler.fit_transform(entradas1)  \n",
    "    \n",
    "        test_X1 = []\n",
    "        for i in range(lags, lags+len(test1)):        \n",
    "            test_X1.append(entradas1[i-lags:i, 0:1])\n",
    "        test_X1 = np.array(test_X1)\n",
    "        \n",
    "        test_X_lstm1 = test_X1\n",
    "        \n",
    "        entradas2 = df2090[len(df2090) - len(test2) - lags:].values\n",
    "        entradas2 = scaler.fit_transform(entradas2)  \n",
    "    \n",
    "        test_X2 = []\n",
    "        for i in range(lags, lags+len(test2)):        \n",
    "            test_X2.append(entradas2[i-lags:i, 0:1])\n",
    "        test_X2 = np.array(test_X2)\n",
    "        \n",
    "        \n",
    "        test_X_lstm2 = test_X2\n",
    "                \n",
    "        #Ajusta a dimensão de test_X e train_X\n",
    "        nsamples, nx, ny = test_X1.shape\n",
    "        test_X1 = test_X1.reshape((nsamples,nx*ny))\n",
    "        \n",
    "        nsamples, nx, ny = test_X2.shape\n",
    "        test_X2 = test_X2.reshape((nsamples,nx*ny))\n",
    "        \n",
    "              \n",
    "        '''\n",
    "        dataset = pd.DataFrame(np.concatenate([X,y], axis=1))\n",
    "        dataset.columns = ['X','y']\n",
    "        dataset.dropna(inplace=True)\n",
    "        '''\n",
    "                        \n",
    "        #Regressão Linear\n",
    "        model = LinearRegression(normalize=False)\n",
    "\n",
    "        model.fit(train_X1,train_y1)\n",
    "        \n",
    "        #Dados de teste\n",
    "        previsoes = model.predict(test_X2).reshape(-1, 1) \n",
    "        previsoes = scaler.inverse_transform(previsoes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>parametro</th>\n",
       "      <th>pH_x</th>\n",
       "      <th>pH_y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data_coleta</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1979-01-31</th>\n",
       "      <td>1.824549</td>\n",
       "      <td>1.894617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979-02-28</th>\n",
       "      <td>1.813738</td>\n",
       "      <td>1.851076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979-03-31</th>\n",
       "      <td>1.791759</td>\n",
       "      <td>1.856298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-01-31</th>\n",
       "      <td>1.890850</td>\n",
       "      <td>1.887070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-02-29</th>\n",
       "      <td>1.840550</td>\n",
       "      <td>1.931521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-31</th>\n",
       "      <td>1.916923</td>\n",
       "      <td>1.931521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-31</th>\n",
       "      <td>1.851599</td>\n",
       "      <td>1.870263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-30</th>\n",
       "      <td>1.921325</td>\n",
       "      <td>1.887070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-30</th>\n",
       "      <td>1.863305</td>\n",
       "      <td>1.871802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-30</th>\n",
       "      <td>1.832581</td>\n",
       "      <td>1.848455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>161 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "parametro        pH_x      pH_y\n",
       "data_coleta                    \n",
       "1979-01-31   1.824549  1.894617\n",
       "1979-02-28   1.813738  1.851076\n",
       "1979-03-31   1.791759  1.856298\n",
       "1980-01-31   1.890850  1.887070\n",
       "1980-02-29   1.840550  1.931521\n",
       "...               ...       ...\n",
       "2018-05-31   1.916923  1.931521\n",
       "2018-08-31   1.851599  1.870263\n",
       "2018-09-30   1.921325  1.887070\n",
       "2018-11-30   1.863305  1.871802\n",
       "2019-09-30   1.832581  1.848455\n",
       "\n",
       "[161 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DFmerged3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-e8bd04f8746b>:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  DFmerged['data_coleta'] = pd.to_datetime(DFmerged['data_coleta'])\n",
      "<ipython-input-19-e8bd04f8746b>:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  DFmerged['data_coleta'] = pd.to_datetime(DFmerged['data_coleta'])\n",
      "<ipython-input-19-e8bd04f8746b>:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  DFmerged['data_coleta'] = pd.to_datetime(DFmerged['data_coleta'])\n",
      "<ipython-input-19-e8bd04f8746b>:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  DFmerged['data_coleta'] = pd.to_datetime(DFmerged['data_coleta'])\n",
      "<ipython-input-19-e8bd04f8746b>:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  DFmerged['data_coleta'] = pd.to_datetime(DFmerged['data_coleta'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coliformes Termotolerantes\n",
      "       cod_interaguas         valor  Altitude  Lat\n",
      "count           245.0    245.000000     245.0  0.0\n",
      "mean            110.0    485.420408     750.0  NaN\n",
      "std               0.0   2279.869695       0.0  NaN\n",
      "min             110.0      0.000000     750.0  NaN\n",
      "25%             110.0      8.000000     750.0  NaN\n",
      "50%             110.0     30.000000     750.0  NaN\n",
      "75%             110.0    130.000000     750.0  NaN\n",
      "max             110.0  28000.000000     750.0  NaN\n",
      "Oxigênio Dissolvido\n",
      "       cod_interaguas       valor  Altitude  Lat\n",
      "count           289.0  289.000000     289.0  0.0\n",
      "mean            110.0    4.599308     750.0  NaN\n",
      "std               0.0    2.263334       0.0  NaN\n",
      "min             110.0    0.000000     750.0  NaN\n",
      "25%             110.0    2.720000     750.0  NaN\n",
      "50%             110.0    4.900000     750.0  NaN\n",
      "75%             110.0    6.330000     750.0  NaN\n",
      "max             110.0   18.000000     750.0  NaN\n",
      "Turbidez\n",
      "       cod_interaguas       valor  Altitude  Lat\n",
      "count           286.0  286.000000     286.0  0.0\n",
      "mean            110.0    8.812413     750.0  NaN\n",
      "std               0.0   11.072338       0.0  NaN\n",
      "min             110.0    0.000000     750.0  NaN\n",
      "25%             110.0    2.900000     750.0  NaN\n",
      "50%             110.0    4.700000     750.0  NaN\n",
      "75%             110.0    9.000000     750.0  NaN\n",
      "max             110.0   75.000000     750.0  NaN\n",
      "Fósforo Total\n",
      "       cod_interaguas       valor  Altitude  Lat\n",
      "count           289.0  289.000000     289.0  0.0\n",
      "mean            110.0    0.059979     750.0  NaN\n",
      "std               0.0    0.060543       0.0  NaN\n",
      "min             110.0    0.005000     750.0  NaN\n",
      "25%             110.0    0.025000     750.0  NaN\n",
      "50%             110.0    0.040000     750.0  NaN\n",
      "75%             110.0    0.070000     750.0  NaN\n",
      "max             110.0    0.491000     750.0  NaN\n",
      "Sólido Total\n",
      "       cod_interaguas       valor  Altitude  Lat\n",
      "count           289.0  289.000000     289.0  0.0\n",
      "mean            110.0   65.429066     750.0  NaN\n",
      "std               0.0   51.381701       0.0  NaN\n",
      "min             110.0    8.000000     750.0  NaN\n",
      "25%             110.0   33.000000     750.0  NaN\n",
      "50%             110.0   48.000000     750.0  NaN\n",
      "75%             110.0  100.000000     750.0  NaN\n",
      "max             110.0  684.000000     750.0  NaN\n",
      "DBO (5, 20)\n",
      "       cod_interaguas       valor  Altitude  Lat\n",
      "count           287.0  287.000000     287.0  0.0\n",
      "mean            110.0    2.888850     750.0  NaN\n",
      "std               0.0    2.051662       0.0  NaN\n",
      "min             110.0    0.100000     750.0  NaN\n",
      "25%             110.0    2.000000     750.0  NaN\n",
      "50%             110.0    3.000000     750.0  NaN\n",
      "75%             110.0    3.000000     750.0  NaN\n",
      "max             110.0   19.000000     750.0  NaN\n",
      "Temperatura da Água\n",
      "       cod_interaguas       valor  Altitude  Lat\n",
      "count           288.0  288.000000     288.0  0.0\n",
      "mean            110.0   20.865069     750.0  NaN\n",
      "std               0.0    2.541333       0.0  NaN\n",
      "min             110.0   15.000000     750.0  NaN\n",
      "25%             110.0   19.000000     750.0  NaN\n",
      "50%             110.0   21.000000     750.0  NaN\n",
      "75%             110.0   23.000000     750.0  NaN\n",
      "max             110.0   29.000000     750.0  NaN\n",
      "pH\n",
      "       cod_interaguas       valor  Altitude         Lat\n",
      "count           289.0  289.000000     289.0       289.0\n",
      "mean            110.0    6.364671     750.0 -23565000.0\n",
      "std               0.0    0.409846       0.0         0.0\n",
      "min             110.0    5.200000     750.0 -23565000.0\n",
      "25%             110.0    6.200000     750.0 -23565000.0\n",
      "50%             110.0    6.370000     750.0 -23565000.0\n",
      "75%             110.0    6.600000     750.0 -23565000.0\n",
      "max             110.0    7.720000     750.0 -23565000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-e8bd04f8746b>:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  DFmerged['data_coleta'] = pd.to_datetime(DFmerged['data_coleta'])\n",
      "<ipython-input-19-e8bd04f8746b>:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  DFmerged['data_coleta'] = pd.to_datetime(DFmerged['data_coleta'])\n",
      "<ipython-input-19-e8bd04f8746b>:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  DFmerged['data_coleta'] = pd.to_datetime(DFmerged['data_coleta'])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "parameters = ['coliformes', 'od', 'turbidez', 'fosforo', 'solido', 'dbo', 'temperatura', 'ph']\n",
    "df_resultado = pd.DataFrame(columns=['tecnica','parametro','amostras','mape','lag'])\n",
    "\n",
    "DFmerge = pd.DataFrame()\n",
    "for parameter in parameters:\n",
    "    # Concatenating all the parameter files\n",
    "    df = pd.read_csv('../../../data/CETESB/'+ parameter + '.csv',encoding='utf-8',sep=';')\n",
    "    DFmerge = pd.concat([DFmerge, df])\n",
    "    \n",
    "    \n",
    "for parameter in DFmerge['parametro'].unique():    \n",
    "    DFmerged = DFmerge[(DFmerge['codigo_ponto'] == 'TIET02050') &\n",
    "                      (DFmerge['parametro'] == parameter)] \n",
    "    \n",
    "    DFmerged['data_coleta'] = pd.to_datetime(DFmerged['data_coleta'])\n",
    "    DFmerged.index = DFmerged['data_coleta']  \n",
    "    DFmerged1 = DFmerged.groupby([pd.Grouper(freq='1M'), 'parametro'])['valor'].mean().unstack()\n",
    "    #DFmerged2 = DFmerged.groupby([pd.Grouper(freq='2M'), 'parametro'])['valor'].mean().unstack()\n",
    "    #DFmerged3 = DFmerged.groupby([pd.Grouper(freq='3M'), 'parametro'])['valor'].mean().unstack()\n",
    "        \n",
    "    print(parameter)\n",
    "    #print(len(DFmerged1))\n",
    "    '''\n",
    "    plt.plot(DFmerged1)\n",
    "    #plt.plot(DFmerged1,label='Trimestral')\n",
    "    #plt.plot(DFmerged2,label='Anual')\n",
    "    plt.xlabel('Data Coleta')\n",
    "    plt.ylabel('Valor')\n",
    "    plt.title('Tendências do parâmetro ' + parameter)\n",
    "    #plt.legend()\n",
    "    plt.show()\n",
    "    '''\n",
    "        \n",
    "    dataframe1 = DFmerged.describe()\n",
    "    print(dataframe1)\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8500283775092636"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.log(6.36 + 0.000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
